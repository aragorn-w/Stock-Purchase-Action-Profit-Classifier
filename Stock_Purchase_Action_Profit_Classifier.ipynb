{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Purchase-Action Profit Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yE5z73wlE6P",
        "outputId": "ebbcd2b5-9777-46cc-ad86-37ce2b2b8c45"
      },
      "source": [
        "!pip install finnhub-python"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting finnhub-python\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/20/97b876250dfda5d55eea48caa1253ab39dc0a8e9b1f75035ccf6ca963276/finnhub_python-2.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from finnhub-python) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (1.24.3)\n",
            "Installing collected packages: finnhub-python\n",
            "Successfully installed finnhub-python-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulBTSZmDpwpL"
      },
      "source": [
        "from time import sleep\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "from pandas.tseries.offsets import BDay\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import finnhub"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vdnC5GuUoVk"
      },
      "source": [
        "# Hyperparameters\n",
        "Hyperparameter | Description\n",
        "---------------|--------------\n",
        "pred_delta_t|Working days from current period to predict (i.e. classifying the up/down trend delta_t days from today)\n",
        "thresh_vec|Vector of non-softmaxed weak, fair, and strong sell or buy discrete probability categories (i.e. stock A on day t0 classifies as a strong sell after delta_t days because it decreases by 26.3%, relative to the price of day t0\n",
        "SML_period_lengths|Vector of WORKING-day lengths for short, medium, and long indicator periods\n",
        "Start timestamp|Earliest historical quote date for the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKXBHiPS7_Lr"
      },
      "source": [
        "pred_delta_t = 15   # measured in working days\n",
        "\n",
        "# NOTE: Change threshold for 'Fair' back to 0.04 after testing with fewer output labels\n",
        "thresh_vec = {0.04: 'Fair', 0.10: 'Moderate', 0.16: 'Strong'}    # inclusive percentage thresholds for confidence in buying and selling\n",
        "sml_period_lengths = [10, 25, 50]    # each element is measured in [working days]\n",
        "\n",
        "start_date = int(datetime(2011, 12, 31).timestamp())   # Dec. 31, 2011\n",
        "end_date = int(datetime(2021, 3, 19).timestamp())   # Mar. 18, 2021\n",
        "\n",
        "api_key = 'c1b6o8v48v6rcdq9ug4g'\n",
        "api_call_time_buffer = 1     # in seconds. Limit is 60 calls a minute\n",
        "fc = finnhub.Client(api_key='c1b6o8v48v6rcdq9ug4g')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmtpMBifb-W1"
      },
      "source": [
        "# Dev Notes\n",
        "## Alpha Vantage\n",
        "Sector requests: https://www.alphavantage.co/query?function=SECTOR&apikey=S7A5K9FH46EIZUQ4  \n",
        "Documentation: https://www.alphavantage.co/documentation/  \n",
        "Client github: https://github.com/RomelTorres/alpha_vantage/blob/develop/alpha_vantage/techindicators.py\n",
        "\n",
        "## Finnhub\n",
        "Documentation: https://finnhub.io/docs/api/recommendation-trends  \n",
        "Technical Indicator API: https://docs.google.com/spreadsheets/d/1ylUvKHVYN2E87WdwIza8ROaCpd48ggEl1k5i5SgA29k/edit#gid=0  \n",
        "Client github: https://github.com/Finnhub-Stock-API/finnhub-python/blob/5f3369e916dde0d5cc34c8d3504feac00781b055/finnhub/client.py\n",
        "\n",
        "## Miscellaneous\n",
        "How to import CSV files in Google Colab: https://www.geeksforgeeks.org/ways-to-import-csv-files-in-google-colab/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1SAMlnJbkHF"
      },
      "source": [
        "# Data Collection and Output Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx6LdN-QtnIY"
      },
      "source": [
        "# Market index static input features (same for every company on the same date)\n",
        "url = 'https://github.com/aragorn-w/Stock-Purchase-Action-Profit-Classifier/raw/main/market-index-data/Nasdaq-Composite.csv'\n",
        "nasdaq = pd.read_csv(url)\n",
        "\n",
        "url = 'https://github.com/aragorn-w/Stock-Purchase-Action-Profit-Classifier/raw/main/market-index-data/Russell-2000.csv'\n",
        "russell = pd.read_csv(url)\n",
        "\n",
        "url = 'https://github.com/aragorn-w/Stock-Purchase-Action-Profit-Classifier/raw/main/market-index-data/SP-500.csv'\n",
        "sp500 = pd.read_csv(url)\n",
        "sp500 = sp500.iloc[::-1]\n",
        "sp500.reset_index(inplace=True, drop=True)\n",
        "sp500.drop(sp500.tail(1).index, inplace=True)\n",
        "sp500.columns = ['Date', 'Open', 'High', 'Low', 'Close'] # cleans up the column names\n",
        "\n",
        "djia_dict = fc.stock_candles('DJIA', 'D', start_date, end_date)\n",
        "dowjones = pd.DataFrame.from_dict(djia_dict)\n",
        "dowjones.columns = ['Close', 'High', 'Low', 'Open', 'Status', 'UNIX Time', 'Volume']\n",
        "\n",
        "market_columns = {}\n",
        "sml_period_lengths = [10, 25, 50]    # each element is measured in [working days]\n",
        "market_indices = {'NasdaqComposite': nasdaq, 'Russell2000': russell, 'SP500': sp500, 'DowJones': dowjones}\n",
        "for market_name, market_index in market_indices.items():\n",
        "    market_columns[market_name + '_Close'] = market_index['Close']\n",
        "    if market_name != 'SP500':\n",
        "        market_columns[market_name + '_Volume'] = market_index['Volume']\n",
        "    for timeperiod in sml_period_lengths:\n",
        "        market_columns[market_name + f'_%change_after_{timeperiod}'] = market_index['Close'].pct_change(periods=timeperiod)\n",
        "market_df = pd.concat(market_columns.values(), axis=1, copy=False)\n",
        "market_df.columns = market_columns.keys()\n",
        "market_df.fillna(value=market_df.mean(), inplace=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m49bBci_VM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4f051fc-27d1-4eae-955c-5f463ddcc382"
      },
      "source": [
        "# Symbols for Training\n",
        "# NOTE: (remember to avoid survivorship bias)\n",
        "training_symbols = ['LSCC', 'A', 'AMZN', 'FB', 'CSCO', 'AAPL', 'MSFT', 'GOOGL', 'ORCL', 'SAP', 'IBM']\n",
        "\n",
        "# Input features\n",
        "# timeperiod_tis is only for features with only ONE timeperiod parameter\n",
        "timeperiod_tis = list(set(['sma', 'ema', 'adx', 'rsi', 'cci', 'wma', 'dema', 'tema', 'trima', \n",
        "                  'kama', 't3', 'willr', 'adxr', 'mom', 'roc', 'rocr', 'aroon', 'aroonosc', \n",
        "                  'mfi', 'trix', 'dx', 'minusdi', 'plusdi', 'minusdm', 'plusdm', 'midprice',\n",
        "                  'atr', 'natr']))\n",
        "# no_timeperiod_ti_columns is for all other features without the criteria of timeperiod_tis\n",
        "no_timeperiod_ti_columns = list(set(['macd', 'ad', 'obv', 'ultosc', 'midpoint', 'sar', 'trange',\n",
        "                                'adosc', 'httrendline', 'httrendmode', 'htdcperiod', 'htdcphase']))\n",
        "misc_columns = ['volume']    # assumes each feature is obtained in the dataset\n",
        "\n",
        "timeperiod_ti_columns = []\n",
        "for ind in timeperiod_tis:\n",
        "    for iii in range(len(thresh_vec)):\n",
        "        timeperiod_ti_columns.append(ind +'_'+str(iii))\n",
        "tech_ind_columns = timeperiod_ti_columns + no_timeperiod_ti_columns\n",
        "input_columns = tech_ind_columns + misc_columns + list(market_columns.keys())\n",
        "\n",
        "# Output features (to be generated by a custom algorithm)\n",
        "# Goes from Strong Sell to Strong Buy, with Minimal Change in the middle\n",
        "label_modifiers = list(thresh_vec.values())\n",
        "output_columns = [s_mod + ' Sell' for s_mod in reversed(label_modifiers)] + ['Hold'] + [s_mod + ' Buy' for s_mod in label_modifiers]\n",
        "\n",
        "all_columns = input_columns + output_columns\n",
        "print('Dataset I/O features:')\n",
        "new_line_ix = 0\n",
        "for col in all_columns:\n",
        "    if new_line_ix != 5:\n",
        "        print(f'{col:<20}', end='')\n",
        "        new_line_ix += 1\n",
        "    else:\n",
        "        print(col)\n",
        "        new_line_ix = 0\n",
        "print('\\n')\n",
        "\n",
        "print('--Total features:', len(all_columns))\n",
        "print('--Input features:', len(input_columns))\n",
        "print('--Output features:', len(output_columns))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset I/O features:\n",
            "plusdm_0            plusdm_1            plusdm_2            trix_0              trix_1              trix_2\n",
            "adx_0               adx_1               adx_2               wma_0               wma_1               wma_2\n",
            "aroonosc_0          aroonosc_1          aroonosc_2          atr_0               atr_1               atr_2\n",
            "willr_0             willr_1             willr_2             dema_0              dema_1              dema_2\n",
            "roc_0               roc_1               roc_2               kama_0              kama_1              kama_2\n",
            "minusdm_0           minusdm_1           minusdm_2           midprice_0          midprice_1          midprice_2\n",
            "aroon_0             aroon_1             aroon_2             trima_0             trima_1             trima_2\n",
            "rsi_0               rsi_1               rsi_2               dx_0                dx_1                dx_2\n",
            "mfi_0               mfi_1               mfi_2               minusdi_0           minusdi_1           minusdi_2\n",
            "rocr_0              rocr_1              rocr_2              ema_0               ema_1               ema_2\n",
            "t3_0                t3_1                t3_2                mom_0               mom_1               mom_2\n",
            "adxr_0              adxr_1              adxr_2              cci_0               cci_1               cci_2\n",
            "tema_0              tema_1              tema_2              natr_0              natr_1              natr_2\n",
            "sma_0               sma_1               sma_2               plusdi_0            plusdi_1            plusdi_2\n",
            "midpoint            adosc               ad                  trange              sar                 httrendline\n",
            "macd                htdcperiod          httrendmode         htdcphase           obv                 ultosc\n",
            "volume              NasdaqComposite_CloseNasdaqComposite_VolumeNasdaqComposite_%change_after_10NasdaqComposite_%change_after_25NasdaqComposite_%change_after_50\n",
            "Russell2000_Close   Russell2000_Volume  Russell2000_%change_after_10Russell2000_%change_after_25Russell2000_%change_after_50SP500_Close\n",
            "SP500_%change_after_10SP500_%change_after_25SP500_%change_after_50DowJones_Close      DowJones_Volume     DowJones_%change_after_10\n",
            "DowJones_%change_after_25DowJones_%change_after_50Strong Sell         Moderate Sell       Fair Sell           Hold\n",
            "Fair Buy            Moderate Buy        Strong Buy          \n",
            "\n",
            "--Total features: 123\n",
            "--Input features: 116\n",
            "--Output features: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQb-3CBSK77h"
      },
      "source": [
        "def full_print(df: pd.DataFrame):\n",
        "    pd.set_option('display.max_columns', 500)\n",
        "    print(df, '\\n')\n",
        "    pd.reset_option('display.max_columns')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AigglRf_XI0I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7E_eDYwDNBD",
        "outputId": "43d45332-7a4e-4dc6-84d0-e985b0b91a4f"
      },
      "source": [
        "tech_ind_col_index = -1\n",
        "def next_tech_ind_col():\n",
        "    global tech_ind_col_index\n",
        "    if tech_ind_col_index < len(tech_ind_columns)-1:\n",
        "        tech_ind_col_index += 1\n",
        "    else:\n",
        "        tech_ind_col_index = 0\n",
        "    return tech_ind_columns[tech_ind_col_index]\n",
        "\n",
        "\n",
        "# Close prices input feature\n",
        "symbol_dataframes = {}\n",
        "for symbol in training_symbols:\n",
        "    candle_data = fc.stock_candles(symbol, 'D', start_date, end_date)\n",
        "    symbol_dataframes[symbol] = pd.DataFrame(candle_data['c'], columns=['future_%_change']) # column label is kept\n",
        "    symbol_dataframes[symbol]['future_%_change'] = symbol_dataframes[symbol]['future_%_change'].pct_change(periods=-pred_delta_t)   # periods is negative to predict future % change\n",
        "    sleep(api_call_time_buffer) # api call buffer\n",
        "\n",
        "    symbol_dataframes[symbol]['volume'] = candle_data['v']\n",
        "\n",
        "# Timeperiod and no-timeperiod technical indicators + timeperiod market index input features\n",
        "for symbol in symbol_dataframes:\n",
        "    for ti in timeperiod_tis:\n",
        "        for sml_period in sml_period_lengths:\n",
        "            ti_data = fc.technical_indicator(symbol, 'D', start_date, end_date, indicator=ti, indicator_fields={'timeperiod': sml_period})\n",
        "            if ti == 'aroon':\n",
        "                ti_data = np.array(ti_data['aroonup']) - np.array(ti_data['aroondown'])\n",
        "            else:\n",
        "                ti_data = np.array(ti_data[ti])\n",
        "            symbol_dataframes[symbol][next_tech_ind_col()] = ti_data\n",
        "            sleep(api_call_time_buffer) # api call buffer\n",
        "    \n",
        "    for ti in no_timeperiod_ti_columns:\n",
        "        ti_data = fc.technical_indicator(symbol, 'D', start_date, end_date, indicator=ti)\n",
        "        if ti == 'macd':\n",
        "            ti_data = np.array(ti_data['macdHist'])\n",
        "        else:\n",
        "            ti_data = np.array(ti_data[ti])\n",
        "        symbol_dataframes[symbol][next_tech_ind_col()] = ti_data\n",
        "        sleep(api_call_time_buffer) # api call buffer\n",
        "\n",
        "    symbol_dataframes[symbol] = pd.concat([symbol_dataframes[symbol], market_df], axis=1, copy=False)\n",
        "    \n",
        "    print(f'{symbol} dataframe obtained')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSCC dataframe obtained\n",
            "A dataframe obtained\n",
            "AMZN dataframe obtained\n",
            "FB dataframe obtained\n",
            "CSCO dataframe obtained\n",
            "AAPL dataframe obtained\n",
            "MSFT dataframe obtained\n",
            "GOOGL dataframe obtained\n",
            "ORCL dataframe obtained\n",
            "SAP dataframe obtained\n",
            "IBM dataframe obtained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGl_5X1-fjX_",
        "collapsed": true
      },
      "source": [
        "# output_columns index for the \"Hold\" label\n",
        "hold_label_index = len(output_columns) // 2\n",
        "def get_action_label(percent_change):\n",
        "    # Function for calculating the correct output label\n",
        "    abs_percent_change = abs(percent_change)\n",
        "    index_shift = 1 if percent_change >= 0.0 else -1\n",
        "    label_index = hold_label_index\n",
        "    for thresh in thresh_vec:\n",
        "        if abs_percent_change >= thresh:\n",
        "            label_index += index_shift    # Strong Sell is leftmost, Strong Buy is rightmost\n",
        "        else:\n",
        "            break\n",
        "    return output_columns[label_index]\n",
        "\n",
        "\n",
        "# Generate output predictions for each SML-period for each company\n",
        "# output_counts = pd.Series(data=0, index=output_columns)\n",
        "for symbol in symbol_dataframes:\n",
        "    symbol_dataframes[symbol]['ACTION_LABEL'] = symbol_dataframes[symbol].apply(lambda row: get_action_label(row['future_%_change']), axis=1)\n",
        "    # output_counts = output_counts.add(symbol_dataframes[symbol]['ACTION_LABEL'].value_counts())\n",
        "    symbol_dataframes[symbol].drop(symbol_dataframes[symbol].tail(pred_delta_t).index, inplace=True)\n",
        "\n",
        "data = pd.concat(symbol_dataframes.values(), ignore_index=True)     # Repeated concatenation to get final dataset\n",
        "plt.show()\n",
        "data = pd.get_dummies(data, columns=['ACTION_LABEL'], prefix='', prefix_sep='')\n",
        "full_print(data.sample(10))\n",
        "assert len(data.columns) == len(all_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ter1ICUeEtes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2fe6c38-3603-46e8-91dd-4758c9417821"
      },
      "source": [
        "# Deal with missing data\n",
        "null_data = data[data.isnull().any(axis=1)]\n",
        "num_nan_rows = len(null_data)\n",
        "print('\\n')\n",
        "print(num_nan_rows, 'rows with NaN in at least one column found')\n",
        "if num_nan_rows != 0:\n",
        "    if num_nan_rows >= 5:\n",
        "        full_print(null_data.sample(5))\n",
        "    else:\n",
        "        full_print(null_data)\n",
        "\n",
        "    # No-crash user prompt for dealing with missing data by dropping the rows or by filling with relative mean\n",
        "    while True:\n",
        "        # print('--------------------\\nA) Drop the rows\\nB) Fill with mean')\n",
        "        # letter_choice = input('Enter your option\\'s letter choice for dealing with the missing data: ').lower()\n",
        "        # if letter_choice == 'a':\n",
        "        #     data.dropna(inplace=True)\n",
        "        # elif letter_choice == 'b':\n",
        "        #     data.fillna(value=data.mean(), inplace=True)\n",
        "        # else:\n",
        "        #     print('Invalid choice, try again.')\n",
        "        #     continue\n",
        "        data.fillna(value=data.mean(), inplace=True)\n",
        "        break"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "94 rows with NaN in at least one column found\n",
            "      future_%_change  volume  atr_0  atr_1  atr_2  rocr_0  rocr_1  rocr_2  \\\n",
            "9155              NaN     NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
            "9202              NaN     NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
            "9205              NaN     NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
            "9204              NaN     NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
            "9145              NaN     NaN    NaN    NaN    NaN     NaN     NaN     NaN   \n",
            "\n",
            "      mfi_0  mfi_1  mfi_2  minusdi_0  minusdi_1  minusdi_2  midprice_0  \\\n",
            "9155    NaN    NaN    NaN        NaN        NaN        NaN         NaN   \n",
            "9202    NaN    NaN    NaN        NaN        NaN        NaN         NaN   \n",
            "9205    NaN    NaN    NaN        NaN        NaN        NaN         NaN   \n",
            "9204    NaN    NaN    NaN        NaN        NaN        NaN         NaN   \n",
            "9145    NaN    NaN    NaN        NaN        NaN        NaN         NaN   \n",
            "\n",
            "      midprice_1  midprice_2  ema_0  ema_1  ema_2  minusdm_0  minusdm_1  \\\n",
            "9155         NaN         NaN    NaN    NaN    NaN        NaN        NaN   \n",
            "9202         NaN         NaN    NaN    NaN    NaN        NaN        NaN   \n",
            "9205         NaN         NaN    NaN    NaN    NaN        NaN        NaN   \n",
            "9204         NaN         NaN    NaN    NaN    NaN        NaN        NaN   \n",
            "9145         NaN         NaN    NaN    NaN    NaN        NaN        NaN   \n",
            "\n",
            "      minusdm_2  kama_0  kama_1  kama_2  mom_0  mom_1  mom_2  plusdi_0  \\\n",
            "9155        NaN     NaN     NaN     NaN    NaN    NaN    NaN       NaN   \n",
            "9202        NaN     NaN     NaN     NaN    NaN    NaN    NaN       NaN   \n",
            "9205        NaN     NaN     NaN     NaN    NaN    NaN    NaN       NaN   \n",
            "9204        NaN     NaN     NaN     NaN    NaN    NaN    NaN       NaN   \n",
            "9145        NaN     NaN     NaN     NaN    NaN    NaN    NaN       NaN   \n",
            "\n",
            "      plusdi_1  plusdi_2  plusdm_0  plusdm_1  plusdm_2  t3_0  t3_1  t3_2  \\\n",
            "9155       NaN       NaN       NaN       NaN       NaN   NaN   NaN   NaN   \n",
            "9202       NaN       NaN       NaN       NaN       NaN   NaN   NaN   NaN   \n",
            "9205       NaN       NaN       NaN       NaN       NaN   NaN   NaN   NaN   \n",
            "9204       NaN       NaN       NaN       NaN       NaN   NaN   NaN   NaN   \n",
            "9145       NaN       NaN       NaN       NaN       NaN   NaN   NaN   NaN   \n",
            "\n",
            "      wma_0  wma_1  wma_2  adxr_0  adxr_1  adxr_2  rsi_0  rsi_1  rsi_2  \\\n",
            "9155    NaN    NaN    NaN     NaN     NaN     NaN    NaN    NaN    NaN   \n",
            "9202    NaN    NaN    NaN     NaN     NaN     NaN    NaN    NaN    NaN   \n",
            "9205    NaN    NaN    NaN     NaN     NaN     NaN    NaN    NaN    NaN   \n",
            "9204    NaN    NaN    NaN     NaN     NaN     NaN    NaN    NaN    NaN   \n",
            "9145    NaN    NaN    NaN     NaN     NaN     NaN    NaN    NaN    NaN   \n",
            "\n",
            "      tema_0  tema_1  tema_2  aroon_0  aroon_1  aroon_2  dx_0  dx_1  dx_2  \\\n",
            "9155     NaN     NaN     NaN      NaN      NaN      NaN   NaN   NaN   NaN   \n",
            "9202     NaN     NaN     NaN      NaN      NaN      NaN   NaN   NaN   NaN   \n",
            "9205     NaN     NaN     NaN      NaN      NaN      NaN   NaN   NaN   NaN   \n",
            "9204     NaN     NaN     NaN      NaN      NaN      NaN   NaN   NaN   NaN   \n",
            "9145     NaN     NaN     NaN      NaN      NaN      NaN   NaN   NaN   NaN   \n",
            "\n",
            "      aroonosc_0  aroonosc_1  aroonosc_2  cci_0  cci_1  cci_2  roc_0  roc_1  \\\n",
            "9155         NaN         NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "9202         NaN         NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "9205         NaN         NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "9204         NaN         NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "9145         NaN         NaN         NaN    NaN    NaN    NaN    NaN    NaN   \n",
            "\n",
            "      roc_2  willr_0  willr_1  willr_2  dema_0  dema_1  dema_2  trix_0  \\\n",
            "9155    NaN      NaN      NaN      NaN     NaN     NaN     NaN     NaN   \n",
            "9202    NaN      NaN      NaN      NaN     NaN     NaN     NaN     NaN   \n",
            "9205    NaN      NaN      NaN      NaN     NaN     NaN     NaN     NaN   \n",
            "9204    NaN      NaN      NaN      NaN     NaN     NaN     NaN     NaN   \n",
            "9145    NaN      NaN      NaN      NaN     NaN     NaN     NaN     NaN   \n",
            "\n",
            "      trix_1  trix_2  natr_0  natr_1  natr_2  adx_0  adx_1  adx_2  sma_0  \\\n",
            "9155     NaN     NaN     NaN     NaN     NaN    NaN    NaN    NaN    NaN   \n",
            "9202     NaN     NaN     NaN     NaN     NaN    NaN    NaN    NaN    NaN   \n",
            "9205     NaN     NaN     NaN     NaN     NaN    NaN    NaN    NaN    NaN   \n",
            "9204     NaN     NaN     NaN     NaN     NaN    NaN    NaN    NaN    NaN   \n",
            "9145     NaN     NaN     NaN     NaN     NaN    NaN    NaN    NaN    NaN   \n",
            "\n",
            "      sma_1  sma_2  trima_0  trima_1  trima_2  midpoint  httrendmode  ad  \\\n",
            "9155    NaN    NaN      NaN      NaN      NaN       NaN          NaN NaN   \n",
            "9202    NaN    NaN      NaN      NaN      NaN       NaN          NaN NaN   \n",
            "9205    NaN    NaN      NaN      NaN      NaN       NaN          NaN NaN   \n",
            "9204    NaN    NaN      NaN      NaN      NaN       NaN          NaN NaN   \n",
            "9145    NaN    NaN      NaN      NaN      NaN       NaN          NaN NaN   \n",
            "\n",
            "      httrendline  ultosc  sar  htdcphase  obv  adosc  trange  macd  \\\n",
            "9155          NaN     NaN  NaN        NaN  NaN    NaN     NaN   NaN   \n",
            "9202          NaN     NaN  NaN        NaN  NaN    NaN     NaN   NaN   \n",
            "9205          NaN     NaN  NaN        NaN  NaN    NaN     NaN   NaN   \n",
            "9204          NaN     NaN  NaN        NaN  NaN    NaN     NaN   NaN   \n",
            "9145          NaN     NaN  NaN        NaN  NaN    NaN     NaN   NaN   \n",
            "\n",
            "      htdcperiod  NasdaqComposite_Close  NasdaqComposite_Volume  \\\n",
            "9155         NaN           12464.230469            5.044210e+09   \n",
            "9202         NaN           14095.469727            7.352960e+09   \n",
            "9205         NaN           13865.360352            6.454280e+09   \n",
            "9204         NaN           13965.490234            7.227140e+09   \n",
            "9145         NaN           11904.709961            5.322580e+09   \n",
            "\n",
            "      NasdaqComposite_%change_after_10  NasdaqComposite_%change_after_25  \\\n",
            "9155                          0.047000                          0.114311   \n",
            "9202                          0.078403                          0.078668   \n",
            "9205                          0.018722                          0.060657   \n",
            "9204                          0.025910                          0.071266   \n",
            "9145                          0.001159                          0.016292   \n",
            "\n",
            "      NasdaqComposite_%change_after_50  Russell2000_Close  Russell2000_Volume  \\\n",
            "9155                          0.167908        1892.449951        5.086370e+09   \n",
            "9202                          0.140861        2289.360107        4.119260e+09   \n",
            "9205                          0.112412        2218.389893        4.773430e+09   \n",
            "9204                          0.128326        2256.110107        4.718280e+09   \n",
            "9145                          0.090216        1784.130005        4.347200e+09   \n",
            "\n",
            "      Russell2000_%change_after_10  Russell2000_%change_after_25  \\\n",
            "9155                      0.060713                      0.211882   \n",
            "9202                      0.104030                      0.091788   \n",
            "9205                      0.027175                      0.042496   \n",
            "9204                      0.048651                      0.078957   \n",
            "9145                      0.074745                      0.088628   \n",
            "\n",
            "      Russell2000_%change_after_50  SP500_Close  SP500_%change_after_10  \\\n",
            "9155                      0.303502      3699.12                0.032734   \n",
            "9202                      0.246894      3934.83                0.059390   \n",
            "9205                      0.172232      3913.97                0.021879   \n",
            "9204                      0.220377      3931.33                0.027447   \n",
            "9145                      0.183306      3581.87                0.020345   \n",
            "\n",
            "      SP500_%change_after_25  SP500_%change_after_50  DowJones_Close  \\\n",
            "9155                0.117522                0.139386      30218.2598   \n",
            "9202                0.034450                0.074371      31458.4004   \n",
            "9205                0.029670                0.058081      31493.3398   \n",
            "9204                0.034667                0.072165      31613.0195   \n",
            "9145                0.028286                0.072676      29483.2305   \n",
            "\n",
            "      DowJones_Volume  DowJones_%change_after_10  DowJones_%change_after_25  \\\n",
            "9155     5.086370e+09                   0.024930                   0.133506   \n",
            "9202     4.119260e+09                   0.049221                   0.013442   \n",
            "9205     4.773430e+09                   0.025054                   0.013668   \n",
            "9204     4.718280e+09                   0.030160                   0.019489   \n",
            "9145     4.347200e+09                   0.038501                   0.034710   \n",
            "\n",
            "      DowJones_%change_after_50  Fair Buy  Fair Sell  Hold  Moderate Buy  \\\n",
            "9155                   0.126898         0          0     1             0   \n",
            "9202                   0.054804         0          0     1             0   \n",
            "9205                   0.042196         0          0     1             0   \n",
            "9204                   0.054839         0          0     1             0   \n",
            "9145                   0.070771         0          0     1             0   \n",
            "\n",
            "      Moderate Sell  Strong Buy  Strong Sell  \n",
            "9155              0           0            0  \n",
            "9202              0           0            0  \n",
            "9205              0           0            0  \n",
            "9204              0           0            0  \n",
            "9145              0           0            0   \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcscNOHFrNyR"
      },
      "source": [
        "# Save the its-not-much-but-its-honest-work-obtained giant dataframe as a csv\n",
        "from google.colab import files      # NOTE: This method is NOT scalable for larger datasets\n",
        "data.to_csv('samuel_900_data.csv')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEZ_RXPfBLpt"
      },
      "source": [
        "# Model Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIZU73ReBQBH"
      },
      "source": [
        "# Load data from notebook's virtual machine file directory\n",
        "data = pd.read_csv('samuel_900_data.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gc1mMEE5YFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4b9bb8b-621e-4118-e118-712d06dc9747"
      },
      "source": [
        "# Split the data into train and test data\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data[input_columns], data[output_columns], test_size=0.25, random_state=42069, shuffle=True)\n",
        "print('Number of examples in training data:', len(train_data))\n",
        "print('Number of examples in testing data:', len(test_data))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of examples in training data: 18999\n",
            "Number of examples in testing data: 6333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iahrga0doYkC"
      },
      "source": [
        "# In-place apply standardization to input features of training and test sets\n",
        "scaler = StandardScaler().fit(train_data)      # NOTE: Use for transforming new data from user-interface loop\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYwXj8va4dAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34797a84-a6ea-4e67-fb1d-3133eb270517"
      },
      "source": [
        "# Make the actual Keras model\n",
        "samuel_900 = keras.models.Sequential(name='SAMUEL_900', layers=[    # weird note, model names in keras can't have spaces\n",
        "    layers.InputLayer(input_shape=[len(input_columns)], name='input_layer'),       # input layer\n",
        "    layers.Dense(116, activation='relu', name='hidden_layer_1'),\n",
        "    layers.Dense(100, activation='relu', name='hidden_layer_2'),\n",
        "    layers.Dense(80, activation='relu', name='hidden_layer_3'),\n",
        "    layers.Dense(80, activation='relu', name='hidden_layer_4'),\n",
        "    layers.Dense(90, activation='relu', name='hidden_layer_5'),\n",
        "    layers.Dense(50, activation='relu', name='hidden_layer_6'),\n",
        "    layers.Dense(len(output_columns), activation='softmax', name='output_layer')    # output layer\n",
        "])\n",
        "samuel_900.summary()\n",
        "samuel_900.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])     # must use categorical_crossentropy for one-hot, sparse version is for binary"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"SAMUEL_900\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer_1 (Dense)       (None, 116)               13572     \n",
            "_________________________________________________________________\n",
            "hidden_layer_2 (Dense)       (None, 100)               11700     \n",
            "_________________________________________________________________\n",
            "hidden_layer_3 (Dense)       (None, 80)                8080      \n",
            "_________________________________________________________________\n",
            "hidden_layer_4 (Dense)       (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "hidden_layer_5 (Dense)       (None, 90)                7290      \n",
            "_________________________________________________________________\n",
            "hidden_layer_6 (Dense)       (None, 50)                4550      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 7)                 357       \n",
            "=================================================================\n",
            "Total params: 52,029\n",
            "Trainable params: 52,029\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuOsm-wbkuBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1275e91-8664-42e9-8f95-7ba38ce7b35a"
      },
      "source": [
        "# Fit the model and track its training history\n",
        "num_epochs = 200    # CONFIG\n",
        "training_batch_size = 64     # CONFIG (ideally powers of 2)\n",
        "history = samuel_900.fit(train_data, train_labels, batch_size=training_batch_size, epochs=num_epochs, shuffle=False, validation_data=(test_data, test_labels))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "297/297 [==============================] - 2s 5ms/step - loss: 1.3378 - accuracy: 0.5402 - val_loss: 1.2288 - val_accuracy: 0.5470\n",
            "Epoch 2/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 1.1866 - accuracy: 0.5562 - val_loss: 1.1737 - val_accuracy: 0.5487\n",
            "Epoch 3/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 1.1173 - accuracy: 0.5668 - val_loss: 1.1323 - val_accuracy: 0.5613\n",
            "Epoch 4/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 1.0411 - accuracy: 0.5858 - val_loss: 1.0873 - val_accuracy: 0.5688\n",
            "Epoch 5/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.9666 - accuracy: 0.6072 - val_loss: 1.0615 - val_accuracy: 0.5767\n",
            "Epoch 6/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.8955 - accuracy: 0.6376 - val_loss: 1.0180 - val_accuracy: 0.5950\n",
            "Epoch 7/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.8293 - accuracy: 0.6648 - val_loss: 1.0071 - val_accuracy: 0.6032\n",
            "Epoch 8/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.7770 - accuracy: 0.6873 - val_loss: 0.9788 - val_accuracy: 0.6297\n",
            "Epoch 9/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.7221 - accuracy: 0.7086 - val_loss: 0.9789 - val_accuracy: 0.6283\n",
            "Epoch 10/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.6664 - accuracy: 0.7342 - val_loss: 0.9779 - val_accuracy: 0.6387\n",
            "Epoch 11/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.7473 - val_loss: 0.9929 - val_accuracy: 0.6283\n",
            "Epoch 12/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.5981 - accuracy: 0.7586 - val_loss: 1.0047 - val_accuracy: 0.6461\n",
            "Epoch 13/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.5664 - accuracy: 0.7727 - val_loss: 1.0082 - val_accuracy: 0.6551\n",
            "Epoch 14/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.5285 - accuracy: 0.7910 - val_loss: 1.0491 - val_accuracy: 0.6525\n",
            "Epoch 15/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.5173 - accuracy: 0.7917 - val_loss: 1.0203 - val_accuracy: 0.6504\n",
            "Epoch 16/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.4947 - accuracy: 0.8021 - val_loss: 1.0277 - val_accuracy: 0.6591\n",
            "Epoch 17/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.4735 - accuracy: 0.8129 - val_loss: 1.0575 - val_accuracy: 0.6581\n",
            "Epoch 18/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.4592 - accuracy: 0.8098 - val_loss: 1.0947 - val_accuracy: 0.6596\n",
            "Epoch 19/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.4320 - accuracy: 0.8236 - val_loss: 1.1333 - val_accuracy: 0.6724\n",
            "Epoch 20/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.4190 - accuracy: 0.8346 - val_loss: 1.1238 - val_accuracy: 0.6689\n",
            "Epoch 21/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.4019 - accuracy: 0.8415 - val_loss: 1.1798 - val_accuracy: 0.6731\n",
            "Epoch 22/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.3814 - accuracy: 0.8438 - val_loss: 1.1838 - val_accuracy: 0.6664\n",
            "Epoch 23/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3723 - accuracy: 0.8506 - val_loss: 1.1928 - val_accuracy: 0.6679\n",
            "Epoch 24/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3603 - accuracy: 0.8554 - val_loss: 1.1906 - val_accuracy: 0.6698\n",
            "Epoch 25/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3534 - accuracy: 0.8598 - val_loss: 1.2289 - val_accuracy: 0.6670\n",
            "Epoch 26/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3435 - accuracy: 0.8631 - val_loss: 1.2572 - val_accuracy: 0.6720\n",
            "Epoch 27/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3401 - accuracy: 0.8669 - val_loss: 1.2970 - val_accuracy: 0.6621\n",
            "Epoch 28/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3384 - accuracy: 0.8646 - val_loss: 1.2403 - val_accuracy: 0.6630\n",
            "Epoch 29/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3349 - accuracy: 0.8680 - val_loss: 1.2139 - val_accuracy: 0.6749\n",
            "Epoch 30/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.3330 - accuracy: 0.8710 - val_loss: 1.1489 - val_accuracy: 0.6754\n",
            "Epoch 31/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3142 - accuracy: 0.8736 - val_loss: 1.1988 - val_accuracy: 0.6676\n",
            "Epoch 32/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3153 - accuracy: 0.8744 - val_loss: 1.1596 - val_accuracy: 0.6705\n",
            "Epoch 33/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.3074 - accuracy: 0.8755 - val_loss: 1.2246 - val_accuracy: 0.6736\n",
            "Epoch 34/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2860 - accuracy: 0.8866 - val_loss: 1.2151 - val_accuracy: 0.6812\n",
            "Epoch 35/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2803 - accuracy: 0.8885 - val_loss: 1.2184 - val_accuracy: 0.6791\n",
            "Epoch 36/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.2743 - accuracy: 0.8907 - val_loss: 1.2481 - val_accuracy: 0.6765\n",
            "Epoch 37/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2896 - accuracy: 0.8856 - val_loss: 1.2873 - val_accuracy: 0.6772\n",
            "Epoch 38/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2756 - accuracy: 0.8881 - val_loss: 1.2400 - val_accuracy: 0.6904\n",
            "Epoch 39/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.2632 - accuracy: 0.8931 - val_loss: 1.2294 - val_accuracy: 0.6776\n",
            "Epoch 40/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.2632 - accuracy: 0.8984 - val_loss: 1.2649 - val_accuracy: 0.6817\n",
            "Epoch 41/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.2601 - accuracy: 0.8993 - val_loss: 1.2061 - val_accuracy: 0.6856\n",
            "Epoch 42/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2498 - accuracy: 0.9017 - val_loss: 1.2680 - val_accuracy: 0.6821\n",
            "Epoch 43/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2585 - accuracy: 0.8999 - val_loss: 1.3120 - val_accuracy: 0.6945\n",
            "Epoch 44/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2567 - accuracy: 0.8997 - val_loss: 1.2122 - val_accuracy: 0.6960\n",
            "Epoch 45/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2374 - accuracy: 0.9062 - val_loss: 1.3574 - val_accuracy: 0.6935\n",
            "Epoch 46/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2374 - accuracy: 0.9050 - val_loss: 1.3674 - val_accuracy: 0.6946\n",
            "Epoch 47/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2316 - accuracy: 0.9114 - val_loss: 1.3826 - val_accuracy: 0.6919\n",
            "Epoch 48/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2261 - accuracy: 0.9118 - val_loss: 1.4131 - val_accuracy: 0.6951\n",
            "Epoch 49/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2251 - accuracy: 0.9118 - val_loss: 1.3510 - val_accuracy: 0.6951\n",
            "Epoch 50/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2138 - accuracy: 0.9153 - val_loss: 1.3912 - val_accuracy: 0.6954\n",
            "Epoch 51/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2250 - accuracy: 0.9130 - val_loss: 1.4155 - val_accuracy: 0.6965\n",
            "Epoch 52/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2099 - accuracy: 0.9195 - val_loss: 1.3515 - val_accuracy: 0.6911\n",
            "Epoch 53/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2111 - accuracy: 0.9160 - val_loss: 1.4257 - val_accuracy: 0.6929\n",
            "Epoch 54/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2038 - accuracy: 0.9199 - val_loss: 1.3400 - val_accuracy: 0.6998\n",
            "Epoch 55/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2005 - accuracy: 0.9255 - val_loss: 1.3780 - val_accuracy: 0.6930\n",
            "Epoch 56/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1999 - accuracy: 0.9252 - val_loss: 1.4193 - val_accuracy: 0.6941\n",
            "Epoch 57/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2195 - accuracy: 0.9192 - val_loss: 1.3633 - val_accuracy: 0.6986\n",
            "Epoch 58/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1933 - accuracy: 0.9260 - val_loss: 1.4309 - val_accuracy: 0.6981\n",
            "Epoch 59/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1893 - accuracy: 0.9261 - val_loss: 1.4631 - val_accuracy: 0.7025\n",
            "Epoch 60/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1762 - accuracy: 0.9314 - val_loss: 1.4484 - val_accuracy: 0.7035\n",
            "Epoch 61/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.2062 - accuracy: 0.9174 - val_loss: 1.4768 - val_accuracy: 0.6934\n",
            "Epoch 62/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1849 - accuracy: 0.9294 - val_loss: 1.4635 - val_accuracy: 0.6870\n",
            "Epoch 63/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1826 - accuracy: 0.9325 - val_loss: 1.5062 - val_accuracy: 0.6957\n",
            "Epoch 64/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1890 - accuracy: 0.9256 - val_loss: 1.6283 - val_accuracy: 0.6823\n",
            "Epoch 65/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1895 - accuracy: 0.9268 - val_loss: 1.5613 - val_accuracy: 0.6937\n",
            "Epoch 66/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1959 - accuracy: 0.9251 - val_loss: 1.5183 - val_accuracy: 0.6949\n",
            "Epoch 67/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1779 - accuracy: 0.9343 - val_loss: 1.5139 - val_accuracy: 0.6941\n",
            "Epoch 68/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1765 - accuracy: 0.9335 - val_loss: 1.6511 - val_accuracy: 0.6894\n",
            "Epoch 69/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1672 - accuracy: 0.9371 - val_loss: 1.4745 - val_accuracy: 0.6900\n",
            "Epoch 70/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1923 - accuracy: 0.9295 - val_loss: 1.5584 - val_accuracy: 0.7049\n",
            "Epoch 71/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1620 - accuracy: 0.9384 - val_loss: 1.6183 - val_accuracy: 0.6941\n",
            "Epoch 72/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1830 - accuracy: 0.9315 - val_loss: 1.5430 - val_accuracy: 0.7074\n",
            "Epoch 73/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1726 - accuracy: 0.9371 - val_loss: 1.5565 - val_accuracy: 0.7055\n",
            "Epoch 74/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1721 - accuracy: 0.9349 - val_loss: 1.5110 - val_accuracy: 0.7008\n",
            "Epoch 75/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1602 - accuracy: 0.9417 - val_loss: 1.5043 - val_accuracy: 0.7028\n",
            "Epoch 76/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1581 - accuracy: 0.9378 - val_loss: 1.6519 - val_accuracy: 0.7031\n",
            "Epoch 77/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1763 - accuracy: 0.9350 - val_loss: 1.5914 - val_accuracy: 0.6971\n",
            "Epoch 78/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1794 - accuracy: 0.9288 - val_loss: 1.6470 - val_accuracy: 0.7036\n",
            "Epoch 79/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1594 - accuracy: 0.9412 - val_loss: 1.5359 - val_accuracy: 0.7011\n",
            "Epoch 80/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1610 - accuracy: 0.9406 - val_loss: 1.5349 - val_accuracy: 0.7082\n",
            "Epoch 81/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1518 - accuracy: 0.9459 - val_loss: 1.6414 - val_accuracy: 0.6976\n",
            "Epoch 82/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1447 - accuracy: 0.9432 - val_loss: 1.6475 - val_accuracy: 0.7003\n",
            "Epoch 83/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1652 - accuracy: 0.9375 - val_loss: 1.5931 - val_accuracy: 0.6994\n",
            "Epoch 84/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1596 - accuracy: 0.9396 - val_loss: 1.5705 - val_accuracy: 0.6952\n",
            "Epoch 85/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1445 - accuracy: 0.9458 - val_loss: 1.6134 - val_accuracy: 0.7008\n",
            "Epoch 86/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1432 - accuracy: 0.9456 - val_loss: 1.6007 - val_accuracy: 0.7076\n",
            "Epoch 87/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1461 - accuracy: 0.9457 - val_loss: 1.6887 - val_accuracy: 0.6976\n",
            "Epoch 88/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1552 - accuracy: 0.9401 - val_loss: 1.7366 - val_accuracy: 0.7050\n",
            "Epoch 89/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1563 - accuracy: 0.9450 - val_loss: 1.6603 - val_accuracy: 0.7000\n",
            "Epoch 90/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1444 - accuracy: 0.9458 - val_loss: 1.6715 - val_accuracy: 0.6934\n",
            "Epoch 91/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1477 - accuracy: 0.9433 - val_loss: 1.6203 - val_accuracy: 0.7069\n",
            "Epoch 92/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1519 - accuracy: 0.9424 - val_loss: 1.6222 - val_accuracy: 0.6965\n",
            "Epoch 93/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1385 - accuracy: 0.9476 - val_loss: 1.7706 - val_accuracy: 0.6981\n",
            "Epoch 94/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1378 - accuracy: 0.9513 - val_loss: 1.6687 - val_accuracy: 0.6990\n",
            "Epoch 95/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1508 - accuracy: 0.9449 - val_loss: 1.6608 - val_accuracy: 0.7008\n",
            "Epoch 96/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1516 - accuracy: 0.9429 - val_loss: 1.7266 - val_accuracy: 0.7031\n",
            "Epoch 97/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1520 - accuracy: 0.9461 - val_loss: 1.7155 - val_accuracy: 0.7011\n",
            "Epoch 98/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1343 - accuracy: 0.9506 - val_loss: 1.6655 - val_accuracy: 0.7000\n",
            "Epoch 99/200\n",
            "297/297 [==============================] - 1s 5ms/step - loss: 0.1426 - accuracy: 0.9467 - val_loss: 1.6522 - val_accuracy: 0.7046\n",
            "Epoch 100/200\n",
            "297/297 [==============================] - 1s 5ms/step - loss: 0.1523 - accuracy: 0.9465 - val_loss: 1.7032 - val_accuracy: 0.7035\n",
            "Epoch 101/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1368 - accuracy: 0.9471 - val_loss: 1.7329 - val_accuracy: 0.7112\n",
            "Epoch 102/200\n",
            "297/297 [==============================] - 1s 5ms/step - loss: 0.1578 - accuracy: 0.9437 - val_loss: 1.7797 - val_accuracy: 0.7074\n",
            "Epoch 103/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1237 - accuracy: 0.9525 - val_loss: 1.7181 - val_accuracy: 0.7042\n",
            "Epoch 104/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1266 - accuracy: 0.9516 - val_loss: 1.6886 - val_accuracy: 0.7009\n",
            "Epoch 105/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1434 - accuracy: 0.9472 - val_loss: 1.6592 - val_accuracy: 0.7088\n",
            "Epoch 106/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1244 - accuracy: 0.9554 - val_loss: 1.7711 - val_accuracy: 0.7036\n",
            "Epoch 107/200\n",
            "297/297 [==============================] - 2s 7ms/step - loss: 0.1511 - accuracy: 0.9451 - val_loss: 1.7233 - val_accuracy: 0.6915\n",
            "Epoch 108/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1358 - accuracy: 0.9510 - val_loss: 1.6807 - val_accuracy: 0.7128\n",
            "Epoch 109/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1305 - accuracy: 0.9553 - val_loss: 1.6848 - val_accuracy: 0.7066\n",
            "Epoch 110/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1187 - accuracy: 0.9546 - val_loss: 1.7611 - val_accuracy: 0.7028\n",
            "Epoch 111/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1186 - accuracy: 0.9588 - val_loss: 1.6849 - val_accuracy: 0.7049\n",
            "Epoch 112/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1522 - accuracy: 0.9469 - val_loss: 1.6357 - val_accuracy: 0.7091\n",
            "Epoch 113/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1391 - accuracy: 0.9512 - val_loss: 1.7285 - val_accuracy: 0.6995\n",
            "Epoch 114/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1404 - accuracy: 0.9489 - val_loss: 1.7931 - val_accuracy: 0.7031\n",
            "Epoch 115/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1253 - accuracy: 0.9537 - val_loss: 1.7385 - val_accuracy: 0.6946\n",
            "Epoch 116/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1225 - accuracy: 0.9549 - val_loss: 1.8897 - val_accuracy: 0.7109\n",
            "Epoch 117/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1322 - accuracy: 0.9535 - val_loss: 1.8597 - val_accuracy: 0.7061\n",
            "Epoch 118/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1200 - accuracy: 0.9574 - val_loss: 1.8137 - val_accuracy: 0.7080\n",
            "Epoch 119/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1363 - accuracy: 0.9522 - val_loss: 1.6462 - val_accuracy: 0.7001\n",
            "Epoch 120/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1283 - accuracy: 0.9542 - val_loss: 1.7674 - val_accuracy: 0.6916\n",
            "Epoch 121/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1388 - accuracy: 0.9510 - val_loss: 1.9050 - val_accuracy: 0.7085\n",
            "Epoch 122/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1345 - accuracy: 0.9516 - val_loss: 1.8275 - val_accuracy: 0.7039\n",
            "Epoch 123/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1140 - accuracy: 0.9579 - val_loss: 1.8870 - val_accuracy: 0.6987\n",
            "Epoch 124/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1107 - accuracy: 0.9582 - val_loss: 1.8608 - val_accuracy: 0.6957\n",
            "Epoch 125/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1246 - accuracy: 0.9528 - val_loss: 1.8413 - val_accuracy: 0.7042\n",
            "Epoch 126/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1198 - accuracy: 0.9566 - val_loss: 1.7731 - val_accuracy: 0.7012\n",
            "Epoch 127/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1121 - accuracy: 0.9587 - val_loss: 1.7465 - val_accuracy: 0.7054\n",
            "Epoch 128/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1070 - accuracy: 0.9599 - val_loss: 1.7627 - val_accuracy: 0.7090\n",
            "Epoch 129/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1194 - accuracy: 0.9594 - val_loss: 1.8166 - val_accuracy: 0.6976\n",
            "Epoch 130/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1245 - accuracy: 0.9516 - val_loss: 1.8482 - val_accuracy: 0.7077\n",
            "Epoch 131/200\n",
            "297/297 [==============================] - 1s 3ms/step - loss: 0.1082 - accuracy: 0.9613 - val_loss: 1.7369 - val_accuracy: 0.7074\n",
            "Epoch 132/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1182 - accuracy: 0.9569 - val_loss: 1.8681 - val_accuracy: 0.6973\n",
            "Epoch 133/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1191 - accuracy: 0.9581 - val_loss: 1.8079 - val_accuracy: 0.7120\n",
            "Epoch 134/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1161 - accuracy: 0.9600 - val_loss: 1.8900 - val_accuracy: 0.6964\n",
            "Epoch 135/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1270 - accuracy: 0.9534 - val_loss: 1.7447 - val_accuracy: 0.7020\n",
            "Epoch 136/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1125 - accuracy: 0.9598 - val_loss: 1.8321 - val_accuracy: 0.7095\n",
            "Epoch 137/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1056 - accuracy: 0.9616 - val_loss: 1.8098 - val_accuracy: 0.7038\n",
            "Epoch 138/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0977 - accuracy: 0.9647 - val_loss: 1.9111 - val_accuracy: 0.6897\n",
            "Epoch 139/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1122 - accuracy: 0.9576 - val_loss: 1.8745 - val_accuracy: 0.6978\n",
            "Epoch 140/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1086 - accuracy: 0.9617 - val_loss: 1.7565 - val_accuracy: 0.6964\n",
            "Epoch 141/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1316 - accuracy: 0.9537 - val_loss: 1.9146 - val_accuracy: 0.7019\n",
            "Epoch 142/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1307 - accuracy: 0.9537 - val_loss: 1.8724 - val_accuracy: 0.6962\n",
            "Epoch 143/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1072 - accuracy: 0.9607 - val_loss: 1.9485 - val_accuracy: 0.6859\n",
            "Epoch 144/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1204 - accuracy: 0.9586 - val_loss: 1.8835 - val_accuracy: 0.7006\n",
            "Epoch 145/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1222 - accuracy: 0.9553 - val_loss: 1.7942 - val_accuracy: 0.6998\n",
            "Epoch 146/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0991 - accuracy: 0.9630 - val_loss: 1.9812 - val_accuracy: 0.6951\n",
            "Epoch 147/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1077 - accuracy: 0.9617 - val_loss: 1.8415 - val_accuracy: 0.7044\n",
            "Epoch 148/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1149 - accuracy: 0.9586 - val_loss: 1.7444 - val_accuracy: 0.6954\n",
            "Epoch 149/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1104 - accuracy: 0.9606 - val_loss: 1.8497 - val_accuracy: 0.7080\n",
            "Epoch 150/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1100 - accuracy: 0.9587 - val_loss: 1.7230 - val_accuracy: 0.7093\n",
            "Epoch 151/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1231 - accuracy: 0.9552 - val_loss: 1.7683 - val_accuracy: 0.6982\n",
            "Epoch 152/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1129 - accuracy: 0.9577 - val_loss: 1.8375 - val_accuracy: 0.7046\n",
            "Epoch 153/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1056 - accuracy: 0.9626 - val_loss: 1.8135 - val_accuracy: 0.6937\n",
            "Epoch 154/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0961 - accuracy: 0.9655 - val_loss: 1.8130 - val_accuracy: 0.7096\n",
            "Epoch 155/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1033 - accuracy: 0.9653 - val_loss: 1.7774 - val_accuracy: 0.7058\n",
            "Epoch 156/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1030 - accuracy: 0.9633 - val_loss: 1.7456 - val_accuracy: 0.7044\n",
            "Epoch 157/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1167 - accuracy: 0.9587 - val_loss: 1.7403 - val_accuracy: 0.7069\n",
            "Epoch 158/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0958 - accuracy: 0.9665 - val_loss: 1.9156 - val_accuracy: 0.6973\n",
            "Epoch 159/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.9539 - val_loss: 1.8292 - val_accuracy: 0.6973\n",
            "Epoch 160/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1012 - accuracy: 0.9637 - val_loss: 1.8133 - val_accuracy: 0.6918\n",
            "Epoch 161/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1062 - accuracy: 0.9604 - val_loss: 1.8267 - val_accuracy: 0.7006\n",
            "Epoch 162/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0983 - accuracy: 0.9635 - val_loss: 1.8343 - val_accuracy: 0.7030\n",
            "Epoch 163/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1078 - accuracy: 0.9620 - val_loss: 1.7217 - val_accuracy: 0.7031\n",
            "Epoch 164/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1134 - accuracy: 0.9601 - val_loss: 1.8297 - val_accuracy: 0.7061\n",
            "Epoch 165/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0999 - accuracy: 0.9659 - val_loss: 1.8364 - val_accuracy: 0.7016\n",
            "Epoch 166/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0906 - accuracy: 0.9662 - val_loss: 1.8254 - val_accuracy: 0.7003\n",
            "Epoch 167/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1357 - accuracy: 0.9543 - val_loss: 1.7920 - val_accuracy: 0.6970\n",
            "Epoch 168/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1091 - accuracy: 0.9626 - val_loss: 1.7923 - val_accuracy: 0.7087\n",
            "Epoch 169/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0946 - accuracy: 0.9656 - val_loss: 1.8373 - val_accuracy: 0.7049\n",
            "Epoch 170/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0986 - accuracy: 0.9664 - val_loss: 1.8366 - val_accuracy: 0.7102\n",
            "Epoch 171/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1040 - accuracy: 0.9620 - val_loss: 1.8133 - val_accuracy: 0.7077\n",
            "Epoch 172/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0889 - accuracy: 0.9673 - val_loss: 1.8240 - val_accuracy: 0.7080\n",
            "Epoch 173/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1100 - accuracy: 0.9634 - val_loss: 1.8220 - val_accuracy: 0.7057\n",
            "Epoch 174/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0850 - accuracy: 0.9705 - val_loss: 1.9413 - val_accuracy: 0.7072\n",
            "Epoch 175/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0902 - accuracy: 0.9683 - val_loss: 1.9698 - val_accuracy: 0.7038\n",
            "Epoch 176/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0987 - accuracy: 0.9659 - val_loss: 1.8280 - val_accuracy: 0.7129\n",
            "Epoch 177/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0977 - accuracy: 0.9645 - val_loss: 1.9003 - val_accuracy: 0.7065\n",
            "Epoch 178/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1031 - accuracy: 0.9639 - val_loss: 1.9117 - val_accuracy: 0.6978\n",
            "Epoch 179/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0996 - accuracy: 0.9642 - val_loss: 1.9213 - val_accuracy: 0.7020\n",
            "Epoch 180/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0943 - accuracy: 0.9642 - val_loss: 2.0236 - val_accuracy: 0.7041\n",
            "Epoch 181/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1348 - accuracy: 0.9547 - val_loss: 1.8751 - val_accuracy: 0.7060\n",
            "Epoch 182/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1071 - accuracy: 0.9637 - val_loss: 1.8391 - val_accuracy: 0.7076\n",
            "Epoch 183/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0841 - accuracy: 0.9707 - val_loss: 1.9424 - val_accuracy: 0.7087\n",
            "Epoch 184/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0860 - accuracy: 0.9711 - val_loss: 1.8224 - val_accuracy: 0.7102\n",
            "Epoch 185/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0927 - accuracy: 0.9671 - val_loss: 1.8747 - val_accuracy: 0.7046\n",
            "Epoch 186/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1113 - accuracy: 0.9616 - val_loss: 1.8364 - val_accuracy: 0.6938\n",
            "Epoch 187/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0853 - accuracy: 0.9682 - val_loss: 1.9880 - val_accuracy: 0.6956\n",
            "Epoch 188/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0992 - accuracy: 0.9664 - val_loss: 1.8713 - val_accuracy: 0.7012\n",
            "Epoch 189/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0915 - accuracy: 0.9680 - val_loss: 1.9589 - val_accuracy: 0.7024\n",
            "Epoch 190/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0988 - accuracy: 0.9653 - val_loss: 1.8660 - val_accuracy: 0.7099\n",
            "Epoch 191/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0870 - accuracy: 0.9678 - val_loss: 2.0559 - val_accuracy: 0.7137\n",
            "Epoch 192/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0791 - accuracy: 0.9712 - val_loss: 1.9689 - val_accuracy: 0.7019\n",
            "Epoch 193/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0968 - accuracy: 0.9667 - val_loss: 1.8287 - val_accuracy: 0.7030\n",
            "Epoch 194/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0962 - accuracy: 0.9669 - val_loss: 1.8661 - val_accuracy: 0.7046\n",
            "Epoch 195/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0842 - accuracy: 0.9692 - val_loss: 1.9213 - val_accuracy: 0.6964\n",
            "Epoch 196/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.1066 - accuracy: 0.9615 - val_loss: 1.8705 - val_accuracy: 0.7082\n",
            "Epoch 197/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0891 - accuracy: 0.9708 - val_loss: 1.9923 - val_accuracy: 0.6952\n",
            "Epoch 198/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0999 - accuracy: 0.9676 - val_loss: 1.9093 - val_accuracy: 0.7079\n",
            "Epoch 199/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0862 - accuracy: 0.9696 - val_loss: 1.9937 - val_accuracy: 0.7076\n",
            "Epoch 200/200\n",
            "297/297 [==============================] - 1s 4ms/step - loss: 0.0873 - accuracy: 0.9713 - val_loss: 2.0565 - val_accuracy: 0.7033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0kNAHQ9nsUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93183029-6800-4d56-8df1-cfd6d007c69d"
      },
      "source": [
        "# Evaluate model training and testing accuracy\n",
        "eval_batch_size = 32\n",
        "train_score, train_accuracy = samuel_900.evaluate(train_data, train_labels, batch_size=eval_batch_size, verbose=0)\n",
        "print('Training loss:', train_score)\n",
        "print(f'\\tTraining accuracy: {train_accuracy}\\n')\n",
        "eval_score, eval_accuracy = samuel_900.evaluate(test_data, test_labels, batch_size=eval_batch_size, verbose=0)\n",
        "print('Testing loss:', eval_score)\n",
        "print(f'\\tTesting accuracy: {eval_accuracy}\\n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.12028215825557709\n",
            "\tTraining accuracy: 0.9604189991950989\n",
            "\n",
            "Testing loss: 2.056476354598999\n",
            "\tTesting accuracy: 0.7033001780509949\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Nc7Y0VpHTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "eff93c18-d761-44a7-ac5e-effa4ccec4ab"
      },
      "source": [
        "# Plot cost and accuracy, copied from Titantic_2\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcDqPa45jGgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b09b60d-038c-43fb-abee-ba0f3469b579"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "samuel_900.save('saved_model/samuel_900')\n",
        "!zip -r /content/saved_model.zip /content/saved_model"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/samuel_900/assets\n",
            "  adding: content/saved_model/ (stored 0%)\n",
            "  adding: content/saved_model/samuel_900/ (stored 0%)\n",
            "  adding: content/saved_model/samuel_900/assets/ (stored 0%)\n",
            "  adding: content/saved_model/samuel_900/variables/ (stored 0%)\n",
            "  adding: content/saved_model/samuel_900/variables/variables.data-00000-of-00001 (deflated 9%)\n",
            "  adding: content/saved_model/samuel_900/variables/variables.index (deflated 59%)\n",
            "  adding: content/saved_model/samuel_900/saved_model.pb (deflated 90%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bkaWsKpbf2l"
      },
      "source": [
        "# User-Interface Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8WlweyFjkoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2053e3-a531-4ddf-b28f-f392d42d7bfa"
      },
      "source": [
        "# Model initialization from notebook virtual machine save\n",
        "samuel_900 = tf.keras.models.load_model('saved_model/samuel_900')\n",
        "samuel_900.summary()     # check its architecture\n",
        "eval_score, eval_accuracy = samuel_900.evaluate(test_data, test_labels, verbose=2)\n",
        "print(f'\\n\\nRestored model score: {eval_score}')\n",
        "print(f'Restored model accuracy: {eval_accuracy}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"SAMUEL_900\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer_1 (Dense)       (None, 116)               13572     \n",
            "_________________________________________________________________\n",
            "hidden_layer_2 (Dense)       (None, 100)               11700     \n",
            "_________________________________________________________________\n",
            "hidden_layer_3 (Dense)       (None, 80)                8080      \n",
            "_________________________________________________________________\n",
            "hidden_layer_4 (Dense)       (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "hidden_layer_5 (Dense)       (None, 90)                7290      \n",
            "_________________________________________________________________\n",
            "hidden_layer_6 (Dense)       (None, 50)                4550      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 7)                 357       \n",
            "=================================================================\n",
            "Total params: 52,029\n",
            "Trainable params: 52,029\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "198/198 - 0s - loss: 2.0565 - accuracy: 0.7033\n",
            "\n",
            "\n",
            "Restored model score: 2.056476354598999\n",
            "Restored model accuracy: 0.7033001780509949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU60yL7hdVv8"
      },
      "source": [
        "def percent_change(old: float, new: float):\n",
        "    if old == 0.0:\n",
        "        old = 0.1\n",
        "    return (new - old) / old\n",
        "\n",
        "\n",
        "def unix_after_bdays(date: datetime, bdays: int) -> int:\n",
        "    date = datetime.today() - BDay(bdays)\n",
        "    return (date - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
        "\n",
        "\n",
        "def get_sample_input(ticker_symbol: str, days_since_last: int=0):\n",
        "    # set days_since_last to 0 for future predictions, pred_delta_t for historical validation\n",
        "    raw_sample_input = []\n",
        "    start_date = unix_after_bdays(datetime.today(), days_since_last+365)\n",
        "    since_last = unix_after_bdays(datetime.today(), days_since_last)\n",
        "    for ti in timeperiod_tis:\n",
        "        for sml_period in sml_period_lengths:\n",
        "            ti_data = fc.technical_indicator(ticker_symbol, 'D', start_date, since_last, indicator=ti, indicator_fields={'timeperiod': sml_period})\n",
        "            if ti == 'aroon':\n",
        "                ti_data = ti_data['aroonup'][-1] - ti_data['aroondown'][-1]\n",
        "            else:\n",
        "                ti_data = ti_data[ti][-1]\n",
        "            raw_sample_input.append(ti_data)\n",
        "            sleep(api_call_time_buffer) # api call buffer\n",
        "    \n",
        "    for ti in no_timeperiod_ti_columns:\n",
        "        ti_data = fc.technical_indicator(ticker_symbol, 'D', start_date, since_last, indicator=ti)\n",
        "        if ti == 'macd':\n",
        "            ti_data = ti_data['macdHist'][-1]\n",
        "        else:\n",
        "            ti_data = ti_data[ti][-1]\n",
        "        raw_sample_input.append(ti_data)\n",
        "        sleep(api_call_time_buffer) # api call buffer\n",
        "\n",
        "    volume = fc.stock_candles(ticker_symbol, 'D', start_date, since_last)['v'][-1]\n",
        "    raw_sample_input.append(volume)\n",
        "\n",
        "    # NOTE: Temporary band-aid for until this can be automated with a stock-index API\n",
        "    # Be careful, there's no user-input safeguards here for simplicity and laziness lol\n",
        "    # for market_name in market_indices:\n",
        "    #     current_price = float(input(f'\\t{market_name}\\'s current price: ').replace(',', ''))\n",
        "    #     for period in sml_period_lengths:\n",
        "    #         period_price = float(input(f'\\t\\t{market_name}\\'s price {period} periods ago: ').replace(',', ''))\n",
        "    #         raw_sample_input.append(percent_change(period_price, current_price))\n",
        "    \n",
        "    # This is the market index data readout for 3/25/2021\n",
        "    raw_sample_input += [12961.89, 6184470000, 13398.67, 13597.97, 13201.98,  # Nasdaq Composite\n",
        "                         2134.27, 4766990000, 2338.54, 2284.38, 2202.98, # Russell 2000\n",
        "                         3889.14, 3939.34, 3925.43, 3824.68, # S&P 500 (does NOT have the volume input feature)\n",
        "                         32420.06, 3993900, 32485.59, 31961.86, 31097.97] # Dow Jones\n",
        "\n",
        "    return np.array(raw_sample_input)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7GBcYTTC49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb36863d-f3f7-4b4b-df25-3a2f8e7579e0"
      },
      "source": [
        "loop_title = 'SAMUEL_900 Purchase-Action UI Loop'\n",
        "equal_sign_separator = '=' * len(loop_title)\n",
        "print(f'{loop_title}\\n{equal_sign_separator}')\n",
        "print('Hi! I\\'m Samuel! I give you my predicted likelihoods for whether you should buy, sell, or hold stock for tickers you request!')\n",
        "print('I specialize in tickers in the information technology sector. I was trained using these symbols:')\n",
        "for ticker_symbol in training_symbols:\n",
        "    print(f'\\t{ticker_symbol}')\n",
        "print()\n",
        "\n",
        "print('Here are the latest confidence-magnitudes for buy and sell actions:')\n",
        "for thresh, label in thresh_vec.items():\n",
        "    print(f'{label:<15}{int(thresh)}%')     # assumes that the confidence magnitudes don't have a thousandth place\n",
        "print()\n",
        "print(f'My latest validation accuracy was {round(eval_accuracy*100.0, 2)}%. Random change is {round(100.0/len(output_columns), 2)}\\n')\n",
        "exit_code = 'exit0'\n",
        "print('Enter your ticker symbols (not case-sensitive), separated by spaces. More tickers takes more time due to my API-call limits.\\n')\n",
        "final_msg = f'When you\\'re finished, just enter \\'{exit_code}\\' to exit and put me to sleep.'\n",
        "equal_sign_separator = '=' * len(final_msg)\n",
        "print(f'{final_msg}\\n{equal_sign_separator}\\n\\n')\n",
        "\n",
        "output_col_index = -1\n",
        "def next_output_col():\n",
        "    global output_col_index\n",
        "    if output_col_index < len(output_columns)-1:\n",
        "        output_col_index += 1\n",
        "    else:\n",
        "        output_col_index = 0\n",
        "    return output_columns[output_col_index]\n",
        "\n",
        "while True:\n",
        "    symbols = input('What ticker symbols would you like me to analyze?\\n').split()\n",
        "    symbols = list(dict.fromkeys(symbols))      # remove any duplicates\n",
        "    print()\n",
        "    if len(symbols) == 1 and symbols[0] == exit_code:\n",
        "        break\n",
        "    for symbol in symbols:\n",
        "        symbol = symbol.upper()\n",
        "        # try:\n",
        "        #     raw_input_data = get_sample_input(symbol)\n",
        "        # except:\n",
        "        #     print('Something went wrong. You probably entered an unknown ticker symbol...\\n')\n",
        "        #     continue\n",
        "        raw_input_data = get_sample_input(symbol)\n",
        "        print(f'{symbol} profitable-action likelihoods:')\n",
        "        symbol_input_data = scaler.transform(raw_input_data.reshape(1, -1))\n",
        "        prediction = samuel_900.predict(symbol_input_data)[0] * 100.0\n",
        "        for prob in prediction:\n",
        "            prob = str(round(prob, 2)) + '%'\n",
        "            print(f'{prob:<15}{next_output_col()}')\n",
        "        print()\n",
        "print('Goodbye.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAMUEL_900 Purchase-Action UI Loop\n",
            "==================================\n",
            "Hi! I'm Samuel! I give you my predicted likelihoods for whether you should buy, sell, or hold stock for tickers you request!\n",
            "I specialize in tickers in the information technology sector. I was trained using these symbols:\n",
            "\tLSCC\n",
            "\tA\n",
            "\tAMZN\n",
            "\tFB\n",
            "\tCSCO\n",
            "\tAAPL\n",
            "\tMSFT\n",
            "\tGOOGL\n",
            "\tORCL\n",
            "\tSAP\n",
            "\tIBM\n",
            "\n",
            "Here are the latest confidence-magnitudes for buy and sell actions:\n",
            "Fair           0%\n",
            "Moderate       0%\n",
            "Strong         0%\n",
            "\n",
            "My latest validation accuracy was 70.33%. Random change is 14.29\n",
            "\n",
            "Enter your ticker symbols (not case-sensitive), separated by spaces. More tickers takes more time due to my API-call limits.\n",
            "\n",
            "When you're finished, just enter 'exit0' to exit and put me to sleep.\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "What ticker symbols would you like me to analyze?\n",
            "aapl\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}