{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Purchase-Action Profit Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yE5z73wlE6P",
        "outputId": "689a2f76-1be2-4124-e8c6-5824f7e5761e"
      },
      "source": [
        "!pip install finnhub-python"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: finnhub-python in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from finnhub-python) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulBTSZmDpwpL"
      },
      "source": [
        "from time import sleep\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "from pandas.tseries.offsets import BDay\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import finnhub"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vdnC5GuUoVk"
      },
      "source": [
        "# Hyperparameters\n",
        "Hyperparameter | Description\n",
        "---------------|--------------\n",
        "pred_delta_t|Working days from current period to predict (i.e. classifying the up/down trend delta_t days from today)\n",
        "thresh_vec|Vector of non-softmaxed weak, fair, and strong sell or buy discrete probability categories (i.e. stock A on day t0 classifies as a strong sell after delta_t days because it decreases by 26.3%, relative to the price of day t0\n",
        "SML_period_lengths|Vector of WORKING-day lengths for short, medium, and long indicator periods\n",
        "Start timestamp|Earliest historical quote date for the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKXBHiPS7_Lr"
      },
      "source": [
        "pred_delta_t = 22   # measured in working days\n",
        "\n",
        "# NOTE: Change threshold for 'Fair' back to 0.04 after testing with fewer output labels\n",
        "thresh_vec = {0.04: 'Fair', 0.10: 'Moderate', 0.16: 'Strong'}    # inclusive percentage thresholds for confidence in buying and selling\n",
        "sml_period_lengths = [10, 25, 50]    # each element is measured in [working days]\n",
        "\n",
        "start_date = int(datetime(2011, 12, 31).timestamp())   # Dec. 31, 2011\n",
        "end_date = int(datetime(2021, 3, 19).timestamp())   # Mar. 18, 2021\n",
        "\n",
        "api_key = 'c1b6o8v48v6rcdq9ug4g'\n",
        "api_call_time_buffer = 1     # in seconds. Limit is 60 calls a minute\n",
        "fc = finnhub.Client(api_key='c1b6o8v48v6rcdq9ug4g')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmtpMBifb-W1"
      },
      "source": [
        "# Dev Notes\n",
        "## Alpha Vantage\n",
        "Sector requests: https://www.alphavantage.co/query?function=SECTOR&apikey=S7A5K9FH46EIZUQ4  \n",
        "Documentation: https://www.alphavantage.co/documentation/  \n",
        "Client github: https://github.com/RomelTorres/alpha_vantage/blob/develop/alpha_vantage/techindicators.py\n",
        "\n",
        "## Finnhub\n",
        "Documentation: https://finnhub.io/docs/api/recommendation-trends  \n",
        "Technical Indicator API: https://docs.google.com/spreadsheets/d/1ylUvKHVYN2E87WdwIza8ROaCpd48ggEl1k5i5SgA29k/edit#gid=0  \n",
        "Client github: https://github.com/Finnhub-Stock-API/finnhub-python/blob/5f3369e916dde0d5cc34c8d3504feac00781b055/finnhub/client.py\n",
        "\n",
        "## Miscellaneous\n",
        "How to import CSV files in Google Colab: https://www.geeksforgeeks.org/ways-to-import-csv-files-in-google-colab/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1SAMlnJbkHF"
      },
      "source": [
        "# Data Collection and Output Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx6LdN-QtnIY"
      },
      "source": [
        "# Market index static input features (same for every company on the same date)\n",
        "url = 'https://github.com/aragorn-w/Stock-Purchase-Action-Profit-Classifier/raw/main/market-index-data/Nasdaq-Composite.csv'\n",
        "nasdaq = pd.read_csv(url)\n",
        "\n",
        "url = 'https://github.com/aragorn-w/Stock-Purchase-Action-Profit-Classifier/raw/main/market-index-data/Russell-2000.csv'\n",
        "russell = pd.read_csv(url)\n",
        "\n",
        "url = 'https://github.com/aragorn-w/Stock-Purchase-Action-Profit-Classifier/raw/main/market-index-data/SP-500.csv'\n",
        "sp500 = pd.read_csv(url)\n",
        "sp500 = sp500.iloc[::-1]\n",
        "sp500.reset_index(inplace=True, drop=True)\n",
        "sp500.drop(sp500.tail(1).index, inplace=True)\n",
        "sp500.columns = ['Date', 'Open', 'High', 'Low', 'Close'] # cleans up the column names\n",
        "\n",
        "djia_dict = fc.stock_candles('DJIA', 'D', start_date, end_date)\n",
        "dowjones = pd.DataFrame.from_dict(djia_dict)\n",
        "dowjones.columns = ['Close', 'High', 'Low', 'Open', 'Status', 'UNIX Time', 'Volume']\n",
        "\n",
        "market_columns = {}\n",
        "sml_period_lengths = [10, 25, 50]    # each element is measured in [working days]\n",
        "market_indices = {'NasdaqComposite': nasdaq, 'Russell2000': russell, 'SP500': sp500, 'DowJones': dowjones}\n",
        "for market_name, market_index in market_indices.items():\n",
        "    market_columns[market_name + '_Close'] = market_index['Close']\n",
        "    if market_name != 'SP500':\n",
        "        market_columns[market_name + '_Volume'] = market_index['Volume']\n",
        "    for timeperiod in sml_period_lengths:\n",
        "        market_columns[market_name + f'_%change_after_{timeperiod}'] = market_index['Close'].pct_change(periods=timeperiod)\n",
        "market_df = pd.concat(market_columns.values(), axis=1, copy=False)\n",
        "market_df.columns = market_columns.keys()\n",
        "market_df.fillna(value=market_df.mean(), inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m49bBci_VM6"
      },
      "source": [
        "# Symbols for Training\n",
        "# NOTE: (remember to avoid survivorship bias)\n",
        "training_symbols = ['LSCC', 'A', 'AMZN', 'FB', 'CSCO', 'AAPL', 'MSFT', 'GOOGL', 'ORCL', 'SAP', 'IBM']\n",
        "\n",
        "# Input features\n",
        "# timeperiod_tis is only for features with only ONE timeperiod parameter\n",
        "timeperiod_tis = list(set(['sma', 'ema', 'adx', 'rsi', 'cci', 'wma', 'dema', 'tema', 'trima', \n",
        "                  'kama', 't3', 'willr', 'adxr', 'mom', 'roc', 'rocr', 'aroon', 'aroonosc', \n",
        "                  'mfi', 'trix', 'dx', 'minusdi', 'plusdi', 'minusdm', 'plusdm', 'midprice',\n",
        "                  'atr', 'natr']))\n",
        "# no_timeperiod_ti_columns is for all other features without the criteria of timeperiod_tis\n",
        "no_timeperiod_ti_columns = list(set(['macd', 'ad', 'obv', 'ultosc', 'midpoint', 'sar', 'trange',\n",
        "                                'adosc', 'httrendline', 'httrendmode', 'htdcperiod', 'htdcphase']))\n",
        "misc_columns = ['volume']    # assumes each feature is obtained in the dataset\n",
        "\n",
        "timeperiod_ti_columns = []\n",
        "for ind in timeperiod_tis:\n",
        "    for iii in range(len(thresh_vec)):\n",
        "        timeperiod_ti_columns.append(ind +'_'+str(iii))\n",
        "tech_ind_columns = timeperiod_ti_columns + no_timeperiod_ti_columns\n",
        "input_columns = tech_ind_columns + misc_columns + list(market_columns.keys())\n",
        "\n",
        "# Output features (to be generated by a custom algorithm)\n",
        "# Goes from Strong Sell to Strong Buy, with Minimal Change in the middle\n",
        "label_modifiers = list(thresh_vec.values())\n",
        "output_columns = [s_mod + ' Sell' for s_mod in reversed(label_modifiers)] + ['Hold'] + [s_mod + ' Buy' for s_mod in label_modifiers]\n",
        "\n",
        "all_columns = input_columns + output_columns\n",
        "print('Dataset I/O features:')\n",
        "new_line_ix = 0\n",
        "for col in all_columns:\n",
        "    if new_line_ix != 5:\n",
        "        print(f'{col:<20}', end='')\n",
        "        new_line_ix += 1\n",
        "    else:\n",
        "        print(col)\n",
        "        new_line_ix = 0\n",
        "print('\\n')\n",
        "\n",
        "print('--Total features:', len(all_columns))\n",
        "print('--Input features:', len(input_columns))\n",
        "print('--Output features:', len(output_columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQb-3CBSK77h"
      },
      "source": [
        "def full_print(df: pd.DataFrame):\n",
        "    pd.set_option('display.max_columns', 500)\n",
        "    print(df, '\\n')\n",
        "    pd.reset_option('display.max_columns')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AigglRf_XI0I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7E_eDYwDNBD"
      },
      "source": [
        "tech_ind_col_index = -1\n",
        "def next_tech_ind_col():\n",
        "    global tech_ind_col_index\n",
        "    if tech_ind_col_index < len(tech_ind_columns)-1:\n",
        "        tech_ind_col_index += 1\n",
        "    else:\n",
        "        tech_ind_col_index = 0\n",
        "    return tech_ind_columns[tech_ind_col_index]\n",
        "\n",
        "\n",
        "# Close prices input feature\n",
        "symbol_dataframes = {}\n",
        "for symbol in training_symbols:\n",
        "    candle_data = fc.stock_candles(symbol, 'D', start_date, end_date)\n",
        "    symbol_dataframes[symbol] = pd.DataFrame(candle_data['c'], columns=['future_%_change']) # column label is kept\n",
        "    symbol_dataframes[symbol]['future_%_change'] = symbol_dataframes[symbol]['future_%_change'].pct_change(periods=-pred_delta_t)   # periods is negative to predict future % change\n",
        "    sleep(api_call_time_buffer) # api call buffer\n",
        "\n",
        "    symbol_dataframes[symbol]['volume'] = candle_data['v']\n",
        "\n",
        "# Timeperiod and no-timeperiod technical indicators + timeperiod market index input features\n",
        "for symbol in symbol_dataframes:\n",
        "    for ti in timeperiod_tis:\n",
        "        for sml_period in sml_period_lengths:\n",
        "            ti_data = fc.technical_indicator(symbol, 'D', start_date, end_date, indicator=ti, indicator_fields={'timeperiod': sml_period})\n",
        "            if ti == 'aroon':\n",
        "                ti_data = np.array(ti_data['aroonup']) - np.array(ti_data['aroondown'])\n",
        "            else:\n",
        "                ti_data = np.array(ti_data[ti])\n",
        "            symbol_dataframes[symbol][next_tech_ind_col()] = ti_data\n",
        "            sleep(api_call_time_buffer) # api call buffer\n",
        "    \n",
        "    for ti in no_timeperiod_ti_columns:\n",
        "        ti_data = fc.technical_indicator(symbol, 'D', start_date, end_date, indicator=ti)\n",
        "        if ti == 'macd':\n",
        "            ti_data = np.array(ti_data['macdHist'])\n",
        "        else:\n",
        "            ti_data = np.array(ti_data[ti])\n",
        "        symbol_dataframes[symbol][next_tech_ind_col()] = ti_data\n",
        "        sleep(api_call_time_buffer) # api call buffer\n",
        "\n",
        "    symbol_dataframes[symbol] = pd.concat([symbol_dataframes[symbol], market_df], axis=1, copy=False)\n",
        "    \n",
        "    print(f'{symbol} dataframe obtained')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGl_5X1-fjX_",
        "collapsed": true
      },
      "source": [
        "# output_columns index for the \"Hold\" label\n",
        "hold_label_index = len(output_columns) // 2\n",
        "def get_action_label(percent_change):\n",
        "    # Function for calculating the correct output label\n",
        "    abs_percent_change = abs(percent_change)\n",
        "    index_shift = 1 if percent_change >= 0.0 else -1\n",
        "    label_index = hold_label_index\n",
        "    for thresh in thresh_vec:\n",
        "        if abs_percent_change >= thresh:\n",
        "            label_index += index_shift    # Strong Sell is leftmost, Strong Buy is rightmost\n",
        "        else:\n",
        "            break\n",
        "    return output_columns[label_index]\n",
        "\n",
        "\n",
        "# Generate output predictions for each SML-period for each company\n",
        "# output_counts = pd.Series(data=0, index=output_columns)\n",
        "for symbol in symbol_dataframes:\n",
        "    symbol_dataframes[symbol]['ACTION_LABEL'] = symbol_dataframes[symbol].apply(lambda row: get_action_label(row['future_%_change']), axis=1)\n",
        "    # output_counts = output_counts.add(symbol_dataframes[symbol]['ACTION_LABEL'].value_counts())\n",
        "    symbol_dataframes[symbol].drop(symbol_dataframes[symbol].tail(pred_delta_t).index, inplace=True)\n",
        "\n",
        "data = pd.concat(symbol_dataframes.values(), ignore_index=True)     # Repeated concatenation to get final dataset\n",
        "plt.show()\n",
        "data = pd.get_dummies(data, columns=['ACTION_LABEL'], prefix='', prefix_sep='')\n",
        "full_print(data.sample(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ter1ICUeEtes"
      },
      "source": [
        "# Deal with missing data\n",
        "null_data = data[data.isnull().any(axis=1)]\n",
        "num_nan_rows = len(null_data)\n",
        "print('\\n')\n",
        "print(num_nan_rows, 'rows with NaN in at least one column found')\n",
        "if num_nan_rows != 0:\n",
        "    if num_nan_rows >= 5:\n",
        "        full_print(null_data.sample(5))\n",
        "    else:\n",
        "        full_print(null_data)\n",
        "\n",
        "    # No-crash user prompt for dealing with missing data by dropping the rows or by filling with relative mean\n",
        "    while True:\n",
        "        # print('--------------------\\nA) Drop the rows\\nB) Fill with mean')\n",
        "        # letter_choice = input('Enter your option\\'s letter choice for dealing with the missing data: ').lower()\n",
        "        # if letter_choice == 'a':\n",
        "        #     data.dropna(inplace=True)\n",
        "        # elif letter_choice == 'b':\n",
        "        #     data.fillna(value=data.mean(), inplace=True)\n",
        "        # else:\n",
        "        #     print('Invalid choice, try again.')\n",
        "        #     continue\n",
        "        data.fillna(value=data.mean(), inplace=True)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcscNOHFrNyR"
      },
      "source": [
        "# Save the its-not-much-but-its-honest-work-obtained giant dataframe as a csv\n",
        "from google.colab import files      # NOTE: This method is NOT scalable for larger datasets\n",
        "data.to_csv('samuel_900_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEZ_RXPfBLpt"
      },
      "source": [
        "# Model Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIZU73ReBQBH"
      },
      "source": [
        "# Load data from notebook's virtual machine file directory\n",
        "data = pd.read_csv('samuel_900_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gc1mMEE5YFj"
      },
      "source": [
        "# Split the data into train and test data\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data[input_columns], data[output_columns], test_size=0.25, random_state=42069, shuffle=True)\n",
        "print('Number of examples in training data:', len(train_data))\n",
        "print('Number of examples in testing data:', len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iahrga0doYkC"
      },
      "source": [
        "# In-place apply standardization to input features of training and test sets\n",
        "scaler = StandardScaler().fit(train_data)      # NOTE: Use for transforming new data from user-interface loop\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYwXj8va4dAO"
      },
      "source": [
        "# Make the actual Keras model\n",
        "samuel_900 = keras.models.Sequential(name='SAMUEL_900', layers=[    # weird note, model names in keras can't have spaces\n",
        "    layers.InputLayer(input_shape=[len(input_columns)], name='input_layer'),       # input layer\n",
        "    layers.Dense(116, activation='relu', name='hidden_layer_1'),\n",
        "    layers.Dense(100, activation='relu', name='hidden_layer_2'),\n",
        "    layers.Dense(80, activation='relu', name='hidden_layer_3'),\n",
        "    layers.Dense(80, activation='relu', name='hidden_layer_4'),\n",
        "    layers.Dense(90, activation='relu', name='hidden_layer_5'),\n",
        "    layers.Dense(50, activation='relu', name='hidden_layer_6'),\n",
        "    layers.Dense(len(output_columns), activation='softmax', name='output_layer')    # output layer\n",
        "])\n",
        "samuel_900.summary()\n",
        "samuel_900.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])     # must use categorical_crossentropy for one-hot, sparse version is for binary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuOsm-wbkuBC"
      },
      "source": [
        "# Fit the model and track its training history\n",
        "num_epochs = 200    # CONFIG\n",
        "training_batch_size = 64     # CONFIG (ideally powers of 2)\n",
        "history = samuel_900.fit(train_data, train_labels, batch_size=training_batch_size, epochs=num_epochs, shuffle=False, validation_data=(test_data, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0kNAHQ9nsUi"
      },
      "source": [
        "# Evaluate model training and testing accuracy\n",
        "eval_batch_size = 32\n",
        "train_score, train_accuracy = samuel_900.evaluate(train_data, train_labels, batch_size=eval_batch_size, verbose=0)\n",
        "print('Training loss:', train_score)\n",
        "print(f'\\tTraining accuracy: {train_accuracy}\\n')\n",
        "eval_score, eval_accuracy = samuel_900.evaluate(test_data, test_labels, batch_size=eval_batch_size, verbose=0)\n",
        "print('Testing loss:', eval_score)\n",
        "print(f'\\tTesting accuracy: {eval_accuracy}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Nc7Y0VpHTK"
      },
      "source": [
        "# Plot cost and accuracy, copied from Titantic_2\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcDqPa45jGgp"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "samuel_900.save('saved_model/samuel_900')\n",
        "!zip -r /content/saved_model.zip /content/saved_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bkaWsKpbf2l"
      },
      "source": [
        "# User-Interface Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8WlweyFjkoB"
      },
      "source": [
        "# Model initialization from notebook virtual machine save\n",
        "samuel_900 = tf.keras.models.load_model('saved_model/samuel_900')\n",
        "samuel_900.summary()     # check its architecture\n",
        "eval_score, eval_accuracy = samuel_900.evaluate(test_data, test_labels, verbose=2)\n",
        "print(f'\\n\\nRestored model score: {eval_score}')\n",
        "print(f'Restored model accuracy: {eval_accuracy}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU60yL7hdVv8"
      },
      "source": [
        "def percent_change(old: float, new: float):\n",
        "    if old == 0.0:\n",
        "        old = 0.1\n",
        "    return (new - old) / old\n",
        "\n",
        "\n",
        "def unix_after_bdays(date: datetime, bdays: int) -> int:\n",
        "    date = datetime.today() - BDay(bdays)\n",
        "    return (date - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
        "\n",
        "\n",
        "def get_sample_input(ticker_symbol: str, days_since_last: int=pred_delta_t):\n",
        "    # set days_since_last to 0 for future predictions, pred_delta_t for historical validation\n",
        "    raw_sample_input = []\n",
        "    start_date = unix_after_bdays(datetime.today(), days_since_last+365)\n",
        "    since_last = unix_after_bdays(datetime.today(), days_since_last)\n",
        "    for ti in timeperiod_tis:\n",
        "        for sml_period in sml_period_lengths:\n",
        "            ti_data = fc.technical_indicator(ticker_symbol, 'D', start_date, since_last, indicator=ti, indicator_fields={'timeperiod': sml_period})\n",
        "            if ti == 'aroon':\n",
        "                ti_data = ti_data['aroonup'][-1] - ti_data['aroondown'][-1]\n",
        "            else:\n",
        "                ti_data = ti_data[ti][-1]\n",
        "            raw_sample_input.append(ti_data)\n",
        "            sleep(api_call_time_buffer) # api call buffer\n",
        "    \n",
        "    for ti in no_timeperiod_ti_columns:\n",
        "        ti_data = fc.technical_indicator(ticker_symbol, 'D', start_date, since_last, indicator=ti)\n",
        "        if ti == 'macd':\n",
        "            ti_data = ti_data['macdHist'][-1]\n",
        "        else:\n",
        "            ti_data = ti_data[ti][-1]\n",
        "        raw_sample_input.append(ti_data)\n",
        "        sleep(api_call_time_buffer) # api call buffer\n",
        "\n",
        "    volume = fc.stock_candles(ticker_symbol, 'D', start_date, since_last)['v'][-1]\n",
        "    raw_sample_input.append(volume)\n",
        "\n",
        "    # NOTE: Temporary band-aid for until this can be automated with a stock-index API\n",
        "    # Be careful, there's no user-input safeguards here for simplicity and laziness lol\n",
        "    # for market_name in market_indices:\n",
        "    #     current_price = float(input(f'\\t{market_name}\\'s current price: ').replace(',', ''))\n",
        "    #     for period in sml_period_lengths:\n",
        "    #         period_price = float(input(f'\\t\\t{market_name}\\'s price {period} periods ago: ').replace(',', ''))\n",
        "    #         raw_sample_input.append(percent_change(period_price, current_price))\n",
        "    \n",
        "    # This is the market index data readout for 3/25/2021\n",
        "    # raw_sample_input += [12961.89, 6184470000, 13398.67, 13597.97, 13201.98,  # Nasdaq Composite\n",
        "    #                      2134.27, 4766990000, 2338.54, 2284.38, 2202.98, # Russell 2000\n",
        "    #                      3889.14, 3939.34, 3925.43, 3824.68, # S&P 500 (does NOT have the volume input feature)\n",
        "    #                      32420.06, 3993900, 32485.59, 31961.86, 31097.97] # Dow Jones\n",
        "\n",
        "    # This is the market index data readout for 2/22/2021 (for predicting 22 work days ahead)\n",
        "    raw_sample_input += [13533.05, 6435100000, 13777.74, 13543.06, 12377.18,\n",
        "                         2251.07, 5870190000, 2202.42, 2141.42, 1848.70,\n",
        "                         3876.50, 3871.74, 3853.07, 3666.72,\n",
        "                         31521.69, 3872100, 31055.86, 31176.01, 29969.52]\n",
        "\n",
        "    return np.array(raw_sample_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7GBcYTTC49"
      },
      "source": [
        "loop_title = 'SAMUEL_900 Purchase-Action UI Loop'\n",
        "equal_sign_separator = '=' * len(loop_title)\n",
        "print(f'{loop_title}\\n{equal_sign_separator}')\n",
        "print('Hi! I\\'m Samuel! I give you my predicted likelihoods for whether you should buy, sell, or hold stock for tickers you request!')\n",
        "print('I specialize in tickers in the information technology sector. I was trained using these symbols:')\n",
        "for ticker_symbol in training_symbols:\n",
        "    print(f'\\t{ticker_symbol}')\n",
        "print()\n",
        "\n",
        "print('Here are the latest confidence-magnitudes for buy and sell actions:')\n",
        "for thresh, label in thresh_vec.items():\n",
        "    print(f'{label:<15}{int(thresh)}%')     # assumes that the confidence magnitudes don't have a thousandth place\n",
        "print()\n",
        "print(f'My latest validation accuracy was {round(eval_accuracy*100.0, 2)}%. Random chance is {round(100.0/len(output_columns), 2)}%\\n')\n",
        "exit_code = 'exit0'\n",
        "print('Enter your ticker symbols (not case-sensitive), separated by spaces. More tickers takes more time due to my API-call limits.\\n')\n",
        "final_msg = f'When you\\'re finished, just enter \\'{exit_code}\\' to exit and put me to sleep.'\n",
        "equal_sign_separator = '=' * len(final_msg)\n",
        "print(f'{final_msg}\\n{equal_sign_separator}\\n\\n')\n",
        "\n",
        "output_col_index = -1\n",
        "def next_output_col():\n",
        "    global output_col_index\n",
        "    if output_col_index < len(output_columns)-1:\n",
        "        output_col_index += 1\n",
        "    else:\n",
        "        output_col_index = 0\n",
        "    return output_columns[output_col_index]\n",
        "\n",
        "while True:\n",
        "    symbols = input('What ticker symbols would you like me to analyze?\\n').split()\n",
        "    symbols = list(dict.fromkeys(symbols))      # remove any duplicates\n",
        "    print()\n",
        "    if len(symbols) == 1 and symbols[0] == exit_code:\n",
        "        break\n",
        "    for symbol in symbols:\n",
        "        symbol = symbol.upper()\n",
        "        # try:\n",
        "        #     raw_input_data = get_sample_input(symbol)\n",
        "        # except:\n",
        "        #     print('Something went wrong. You probably entered an unknown ticker symbol...\\n')\n",
        "        #     continue\n",
        "        raw_input_data = get_sample_input(symbol)\n",
        "        print(f'{symbol} profitable-action likelihoods:')\n",
        "        symbol_input_data = scaler.transform(raw_input_data.reshape(1, -1))\n",
        "        prediction = samuel_900.predict(symbol_input_data)[0] * 100.0\n",
        "        for prob in prediction:\n",
        "            prob = str(round(prob, 2)) + '%'\n",
        "            print(f'{prob:<15}{next_output_col()}')\n",
        "        print()\n",
        "print('Goodbye.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}