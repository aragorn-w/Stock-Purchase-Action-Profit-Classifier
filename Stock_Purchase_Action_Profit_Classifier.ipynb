{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Purchase-Action Profit Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yE5z73wlE6P",
        "outputId": "20be0c9a-fca7-44c8-f02a-928deb2240d5"
      },
      "source": [
        "!pip install finnhub-python"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: finnhub-python in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from finnhub-python) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->finnhub-python) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulBTSZmDpwpL"
      },
      "source": [
        "from time import sleep\n",
        "from datetime import datetime\n",
        "\n",
        "import pandas as pd\n",
        "from pandas.tseries.offsets import BDay\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import finnhub"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vdnC5GuUoVk"
      },
      "source": [
        "# Hyperparameters\n",
        "Hyperparameter | Description\n",
        "---------------|--------------\n",
        "pred_delta_t|Working days from current period to predict (i.e. classifying the up/down trend delta_t days from today)\n",
        "thresh_vec|Vector of non-softmaxed weak, fair, and strong sell or buy discrete probability categories (i.e. stock A on day t0 classifies as a strong sell after delta_t days because it decreases by 26.3%, relative to the price of day t0\n",
        "SML_period_lengths|Vector of WORKING-day lengths for short, medium, and long indicator periods\n",
        "Start timestamp|Earliest historical quote date for the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKXBHiPS7_Lr"
      },
      "source": [
        "pred_delta_t = 30   # measured in working days\n",
        "\n",
        "# NOTE: Change threshold for 'Fair' back to 0.04 after testing with fewer output labels\n",
        "thresh_vec = {0.04: 'Fair', 0.10: 'Moderate', 0.16: 'Strong'}    # inclusive percentage thresholds for confidence in buying and selling\n",
        "sml_period_lengths = [10, 25, 50]    # each element is measured in [working days]\n",
        "\n",
        "start_date = int(datetime(2011, 12, 31).timestamp())   # Dec. 31, 2011\n",
        "end_date = int(datetime(2021, 3, 19).timestamp())   # Mar. 18, 2021\n",
        "\n",
        "api_key = 'c1b6o8v48v6rcdq9ug4g'\n",
        "api_call_time_buffer = 1     # in seconds. Limit is 60 calls a minute\n",
        "fc = finnhub.Client(api_key='c1b6o8v48v6rcdq9ug4g')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmtpMBifb-W1"
      },
      "source": [
        "# Dev Notes\n",
        "## Alpha Vantage\n",
        "Sector requests: https://www.alphavantage.co/query?function=SECTOR&apikey=S7A5K9FH46EIZUQ4  \n",
        "Documentation: https://www.alphavantage.co/documentation/  \n",
        "Client github: https://github.com/RomelTorres/alpha_vantage/blob/develop/alpha_vantage/techindicators.py\n",
        "\n",
        "## Finnhub\n",
        "Documentation: https://finnhub.io/docs/api/recommendation-trends  \n",
        "Technical Indicator API: https://docs.google.com/spreadsheets/d/1ylUvKHVYN2E87WdwIza8ROaCpd48ggEl1k5i5SgA29k/edit#gid=0  \n",
        "Client github: https://github.com/Finnhub-Stock-API/finnhub-python/blob/5f3369e916dde0d5cc34c8d3504feac00781b055/finnhub/client.py\n",
        "\n",
        "## Miscellaneous\n",
        "How to import CSV files in Google Colab: https://www.geeksforgeeks.org/ways-to-import-csv-files-in-google-colab/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1SAMlnJbkHF"
      },
      "source": [
        "# Data Collection and Output Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx6LdN-QtnIY"
      },
      "source": [
        "# Market index static input features (same for every company on the same date)\n",
        "url = 'https://github.com/aragorn-w/Stock-Purchase-Action-Profit-Classifier/raw/main/market-index-data/Nasdaq-Composite.csv'\n",
        "nasdaq = pd.read_csv(url)\n",
        "\n",
        "url = 'https://github.com/aragorn-w/Stock-Purchase-Action-Profit-Classifier/raw/main/market-index-data/Russell-2000.csv'\n",
        "russell = pd.read_csv(url)\n",
        "\n",
        "url = 'https://github.com/aragorn-w/Stock-Purchase-Action-Profit-Classifier/raw/main/market-index-data/SP-500.csv'\n",
        "sp500 = pd.read_csv(url)\n",
        "sp500 = sp500.iloc[::-1]\n",
        "sp500.reset_index(inplace=True, drop=True)\n",
        "sp500.drop(sp500.tail(1).index, inplace=True)\n",
        "sp500.columns = ['Date', 'Open', 'High', 'Low', 'Close'] # cleans up the column names\n",
        "\n",
        "djia_dict = fc.stock_candles('DJIA', 'D', start_date, end_date)\n",
        "dowjones = pd.DataFrame.from_dict(djia_dict)\n",
        "dowjones.columns = ['Close', 'High', 'Low', 'Open', 'Status', 'UNIX Time', 'Volume']\n",
        "\n",
        "market_columns = {}\n",
        "sml_period_lengths = [10, 25, 50]    # each element is measured in [working days]\n",
        "market_indices = {'NasdaqComposite': nasdaq, 'Russell2000': russell, 'SP500': sp500, 'DowJones': dowjones}\n",
        "for market_name, market_index in market_indices.items():\n",
        "    market_columns[market_name + '_Close'] = market_index['Close']\n",
        "    if market_name != 'SP500':\n",
        "        market_columns[market_name + '_Volume'] = market_index['Volume']\n",
        "    for timeperiod in sml_period_lengths:\n",
        "        market_columns[market_name + f'_%change_after_{timeperiod}'] = market_index['Close'].pct_change(periods=timeperiod)\n",
        "market_df = pd.concat(market_columns.values(), axis=1, copy=False)\n",
        "market_df.columns = market_columns.keys()\n",
        "market_df.fillna(value=market_df.mean(), inplace=True)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m49bBci_VM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0589f2a9-0eb1-4b41-ab90-c0f6d6bdf437"
      },
      "source": [
        "# Symbols for Training\n",
        "# NOTE: (remember to avoid survivorship bias)\n",
        "training_symbols = ['LSCC', 'A', 'AMZN', 'FB', 'CSCO', 'AAPL', 'MSFT', 'GOOGL', 'ORCL', 'SAP', 'IBM']\n",
        "\n",
        "# Input features\n",
        "# timeperiod_tis is only for features with only ONE timeperiod parameter\n",
        "timeperiod_tis = list(set(['sma', 'ema', 'adx', 'rsi', 'cci', 'wma', 'dema', 'tema', 'trima', \n",
        "                  'kama', 't3', 'willr', 'adxr', 'mom', 'roc', 'rocr', 'aroon', 'aroonosc', \n",
        "                  'mfi', 'trix', 'dx', 'minusdi', 'plusdi', 'minusdm', 'plusdm', 'midprice',\n",
        "                  'atr', 'natr']))\n",
        "# no_timeperiod_ti_columns is for all other features without the criteria of timeperiod_tis\n",
        "no_timeperiod_ti_columns = list(set(['macd', 'ad', 'obv', 'ultosc', 'midpoint', 'sar', 'trange',\n",
        "                                'adosc', 'httrendline', 'httrendmode', 'htdcperiod', 'htdcphase']))\n",
        "misc_columns = ['volume']    # assumes each feature is obtained in the dataset\n",
        "\n",
        "timeperiod_ti_columns = []\n",
        "for ind in timeperiod_tis:\n",
        "    for iii in range(len(thresh_vec)):\n",
        "        timeperiod_ti_columns.append(ind +'_'+str(iii))\n",
        "tech_ind_columns = timeperiod_ti_columns + no_timeperiod_ti_columns\n",
        "input_columns = tech_ind_columns + misc_columns + list(market_columns.keys())\n",
        "\n",
        "# Output features (to be generated by a custom algorithm)\n",
        "# Goes from Strong Sell to Strong Buy, with Minimal Change in the middle\n",
        "label_modifiers = list(thresh_vec.values())\n",
        "output_columns = [s_mod + ' Sell' for s_mod in reversed(label_modifiers)] + ['Hold'] + [s_mod + ' Buy' for s_mod in label_modifiers]\n",
        "\n",
        "all_columns = input_columns + output_columns\n",
        "print('Dataset I/O features:')\n",
        "new_line_ix = 0\n",
        "for col in all_columns:\n",
        "    if new_line_ix != 5:\n",
        "        print(f'{col:<20}', end='')\n",
        "        new_line_ix += 1\n",
        "    else:\n",
        "        print(col)\n",
        "        new_line_ix = 0\n",
        "print('\\n')\n",
        "\n",
        "print('--Total features:', len(all_columns))\n",
        "print('--Input features:', len(input_columns))\n",
        "print('--Output features:', len(output_columns))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset I/O features:\n",
            "plusdm_0            plusdm_1            plusdm_2            trix_0              trix_1              trix_2\n",
            "adx_0               adx_1               adx_2               wma_0               wma_1               wma_2\n",
            "aroonosc_0          aroonosc_1          aroonosc_2          atr_0               atr_1               atr_2\n",
            "willr_0             willr_1             willr_2             dema_0              dema_1              dema_2\n",
            "roc_0               roc_1               roc_2               kama_0              kama_1              kama_2\n",
            "minusdm_0           minusdm_1           minusdm_2           midprice_0          midprice_1          midprice_2\n",
            "aroon_0             aroon_1             aroon_2             trima_0             trima_1             trima_2\n",
            "rsi_0               rsi_1               rsi_2               dx_0                dx_1                dx_2\n",
            "mfi_0               mfi_1               mfi_2               minusdi_0           minusdi_1           minusdi_2\n",
            "rocr_0              rocr_1              rocr_2              ema_0               ema_1               ema_2\n",
            "t3_0                t3_1                t3_2                mom_0               mom_1               mom_2\n",
            "adxr_0              adxr_1              adxr_2              cci_0               cci_1               cci_2\n",
            "tema_0              tema_1              tema_2              natr_0              natr_1              natr_2\n",
            "sma_0               sma_1               sma_2               plusdi_0            plusdi_1            plusdi_2\n",
            "midpoint            adosc               ad                  trange              sar                 httrendline\n",
            "macd                htdcperiod          httrendmode         htdcphase           obv                 ultosc\n",
            "volume              NasdaqComposite_CloseNasdaqComposite_VolumeNasdaqComposite_%change_after_10NasdaqComposite_%change_after_25NasdaqComposite_%change_after_50\n",
            "Russell2000_Close   Russell2000_Volume  Russell2000_%change_after_10Russell2000_%change_after_25Russell2000_%change_after_50SP500_Close\n",
            "SP500_%change_after_10SP500_%change_after_25SP500_%change_after_50DowJones_Close      DowJones_Volume     DowJones_%change_after_10\n",
            "DowJones_%change_after_25DowJones_%change_after_50Strong Sell         Moderate Sell       Fair Sell           Hold\n",
            "Fair Buy            Moderate Buy        Strong Buy          \n",
            "\n",
            "--Total features: 123\n",
            "--Input features: 116\n",
            "--Output features: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQb-3CBSK77h"
      },
      "source": [
        "def full_print(df: pd.DataFrame):\n",
        "    pd.set_option('display.max_columns', 500)\n",
        "    print(df, '\\n')\n",
        "    pd.reset_option('display.max_columns')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AigglRf_XI0I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7E_eDYwDNBD",
        "outputId": "2d9c9d16-85e4-4449-ba9f-0f89eeb95dbb"
      },
      "source": [
        "tech_ind_col_index = -1\n",
        "def next_tech_ind_col():\n",
        "    global tech_ind_col_index\n",
        "    if tech_ind_col_index < len(tech_ind_columns)-1:\n",
        "        tech_ind_col_index += 1\n",
        "    else:\n",
        "        tech_ind_col_index = 0\n",
        "    return tech_ind_columns[tech_ind_col_index]\n",
        "\n",
        "\n",
        "# Close prices input feature\n",
        "symbol_dataframes = {}\n",
        "for symbol in training_symbols:\n",
        "    candle_data = fc.stock_candles(symbol, 'D', start_date, end_date)\n",
        "    symbol_dataframes[symbol] = pd.DataFrame(candle_data['c'], columns=['future_%_change']) # column label is kept\n",
        "    symbol_dataframes[symbol]['future_%_change'] = symbol_dataframes[symbol]['future_%_change'].pct_change(periods=-pred_delta_t)   # periods is negative to predict future % change\n",
        "    sleep(api_call_time_buffer) # api call buffer\n",
        "\n",
        "    symbol_dataframes[symbol]['volume'] = candle_data['v']\n",
        "\n",
        "# Timeperiod and no-timeperiod technical indicators + timeperiod market index input features\n",
        "for symbol in symbol_dataframes:\n",
        "    for ti in timeperiod_tis:\n",
        "        for sml_period in sml_period_lengths:\n",
        "            ti_data = fc.technical_indicator(symbol, 'D', start_date, end_date, indicator=ti, indicator_fields={'timeperiod': sml_period})\n",
        "            if ti == 'aroon':\n",
        "                ti_data = np.array(ti_data['aroonup']) - np.array(ti_data['aroondown'])\n",
        "            else:\n",
        "                ti_data = np.array(ti_data[ti])\n",
        "            symbol_dataframes[symbol][next_tech_ind_col()] = ti_data\n",
        "            sleep(api_call_time_buffer) # api call buffer\n",
        "    \n",
        "    for ti in no_timeperiod_ti_columns:\n",
        "        ti_data = fc.technical_indicator(symbol, 'D', start_date, end_date, indicator=ti)\n",
        "        if ti == 'macd':\n",
        "            ti_data = np.array(ti_data['macdHist'])\n",
        "        else:\n",
        "            ti_data = np.array(ti_data[ti])\n",
        "        symbol_dataframes[symbol][next_tech_ind_col()] = ti_data\n",
        "        sleep(api_call_time_buffer) # api call buffer\n",
        "\n",
        "    symbol_dataframes[symbol] = pd.concat([symbol_dataframes[symbol], market_df], axis=1, copy=False)\n",
        "    \n",
        "    print(f'{symbol} dataframe obtained')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSCC dataframe obtained\n",
            "A dataframe obtained\n",
            "AMZN dataframe obtained\n",
            "FB dataframe obtained\n",
            "CSCO dataframe obtained\n",
            "AAPL dataframe obtained\n",
            "MSFT dataframe obtained\n",
            "GOOGL dataframe obtained\n",
            "ORCL dataframe obtained\n",
            "SAP dataframe obtained\n",
            "IBM dataframe obtained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGl_5X1-fjX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "50f02c62-e1aa-4c6a-eba4-0e96dc259d8d"
      },
      "source": [
        "# output_columns index for the \"Hold\" label\n",
        "hold_label_index = len(output_columns) // 2\n",
        "def get_action_label(percent_change):\n",
        "    # Function for calculating the correct output label\n",
        "    abs_percent_change = abs(percent_change)\n",
        "    index_shift = 1 if percent_change >= 0.0 else -1\n",
        "    label_index = hold_label_index\n",
        "    for thresh in thresh_vec:\n",
        "        if abs_percent_change >= thresh:\n",
        "            label_index += index_shift    # Strong Sell is leftmost, Strong Buy is rightmost\n",
        "        else:\n",
        "            break\n",
        "    return output_columns[label_index]\n",
        "\n",
        "\n",
        "# Generate output predictions for each SML-period for each company\n",
        "# output_counts = pd.Series(data=0, index=output_columns)\n",
        "for symbol in symbol_dataframes:\n",
        "    symbol_dataframes[symbol]['ACTION_LABEL'] = symbol_dataframes[symbol].apply(lambda row: get_action_label(row['future_%_change']), axis=1)\n",
        "    # output_counts = output_counts.add(symbol_dataframes[symbol]['ACTION_LABEL'].value_counts())\n",
        "    symbol_dataframes[symbol].drop(symbol_dataframes[symbol].tail(pred_delta_t).index, inplace=True)\n",
        "\n",
        "data = pd.concat(symbol_dataframes.values(), ignore_index=True)     # Repeated concatenation to get final dataset\n",
        "plt.show()\n",
        "data = pd.get_dummies(data, columns=['ACTION_LABEL'], prefix='', prefix_sep='')\n",
        "full_print(data.sample(10))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       future_%_change      volume  plusdm_0   plusdm_1   plusdm_2    trix_0  \\\n",
            "21449        -0.157637   1078106.0  5.373800  10.764924  19.750165 -0.220139   \n",
            "1153         -0.082043   1034145.0  0.328237   1.112305   2.571423  0.399592   \n",
            "1808          0.018011   1801427.0  1.566652   4.030513   6.648222  1.521598   \n",
            "12160         0.113137  34039345.0  0.700633   2.383976   5.837799 -0.091132   \n",
            "18531        -0.079331  16146034.0  1.541565   3.337147   6.428197  0.172714   \n",
            "7496          0.019288  19062934.0  3.435077  10.466375  20.697298  0.228151   \n",
            "21265        -0.145585    687031.0  4.414438   8.460458  15.706604 -0.185016   \n",
            "2812          0.039655   1804350.0  1.895331   4.858944   9.687349 -0.099010   \n",
            "18935         0.052220  17394280.0  1.632929   3.956055   8.053886 -0.115260   \n",
            "21994         0.023706    394787.0  7.608594  15.576133  28.158832  0.095852   \n",
            "\n",
            "         trix_1    trix_2      adx_0      adx_1      adx_2       wma_0  \\\n",
            "21449 -0.009802  0.020967  28.011387  11.781293   5.622717   76.128364   \n",
            "1153   0.142398  0.048719  17.095592   8.532999   5.356988    6.055091   \n",
            "1808   1.380995  0.473843  67.122910  43.323749  22.492125   12.732727   \n",
            "12160  0.041989  0.109183  26.094543  10.300275   7.478784   31.861227   \n",
            "18531  0.050541  0.013635  30.613850  11.407707   5.875591   34.127455   \n",
            "7496   0.141799  0.063796  20.926442  11.955246   6.339266   82.522545   \n",
            "21265 -0.220090 -0.092440  23.590713  23.397678  13.948665   64.767636   \n",
            "2812   0.008644  0.137504  12.821549  12.142993  15.748345   40.432198   \n",
            "18935  0.008270  0.029343  23.373690   8.795119   4.958645   42.826727   \n",
            "21994  0.011150  0.069657  22.553984   8.675334   6.035450  119.107455   \n",
            "\n",
            "            wma_1       wma_2  aroonosc_0  aroonosc_1  aroonosc_2     atr_0  \\\n",
            "21449   77.688985   78.266306       -20.0       -56.0       -28.0  2.085798   \n",
            "1153     5.985354    5.825051        50.0        80.0        42.0  0.201780   \n",
            "1808    12.161415   10.564769        80.0        80.0        94.0  0.454249   \n",
            "12160   32.109781   32.145458        20.0       -44.0       -12.0  0.407910   \n",
            "18531   33.645508   33.427537       100.0        96.0       100.0  0.497083   \n",
            "7496    81.862277   80.379502       -80.0        36.0        60.0  1.659687   \n",
            "21265   64.993569   66.251404        70.0       -28.0       -86.0  1.247509   \n",
            "2812    40.691403   41.075293       -60.0        -8.0       -34.0  0.832972   \n",
            "18935   43.362338   43.620251       -10.0        -4.0        -2.0  0.993577   \n",
            "21994  117.152031  117.264149        80.0        32.0       -40.0  1.518575   \n",
            "\n",
            "          atr_1     atr_2    willr_0    willr_1    willr_2      dema_0  \\\n",
            "21449  1.588863  1.424235 -74.872319 -77.222222 -77.222222   74.714926   \n",
            "1153   0.214979  0.237835 -83.146067 -30.452675 -26.241135    6.068554   \n",
            "1808   0.427868  0.379626 -12.056738  -3.096539  -2.619414   13.112437   \n",
            "12160  0.485307  0.534455 -56.359649 -72.925170 -61.985689   31.775919   \n",
            "18531  0.509174  0.534962 -10.416667  -5.263158  -5.000000   34.437268   \n",
            "7496   1.664427  1.691143 -68.879865 -41.202511 -28.763302   82.632177   \n",
            "21265  1.297329  1.281358  -3.073286 -24.493554 -61.718021   65.088654   \n",
            "2812   0.842488  0.794437 -65.770848 -68.514534 -72.282417   40.288174   \n",
            "18935  0.844961  0.798143 -82.669789 -82.669789 -82.669789   42.182012   \n",
            "21994  1.605097  1.639369  -1.656920  -1.656920  -4.990584  120.402568   \n",
            "\n",
            "           dema_1      dema_2     roc_0      roc_1      roc_2      kama_0  \\\n",
            "21449   76.605951   77.841448 -2.969386  -5.259782  -6.919587   77.264692   \n",
            "1153     6.075891    5.950036 -4.662379  16.274510   7.622505    6.033684   \n",
            "1808    13.160671   12.022834  7.438017  65.394402  90.895742   12.398625   \n",
            "12160   32.025009   32.413089 -0.148670  -0.900831   0.401259   32.113714   \n",
            "18531   34.004551   33.687880  2.936814   7.198023   8.369769   33.880609   \n",
            "7496    82.930940   81.922681 -1.622912   4.394074   6.168706   82.148695   \n",
            "21265   64.071723   64.574468  2.427408  -1.171064  -7.952506   65.428132   \n",
            "2812    40.545438   41.364352 -2.024098  -5.190086  -0.017530   41.412516   \n",
            "18935   43.033410   43.514953 -3.157650  -5.718807  -3.180130   43.446210   \n",
            "21994  118.156042  118.338503  7.107800   4.622892   0.849975  118.973317   \n",
            "\n",
            "           kama_1      kama_2  minusdm_0  minusdm_1  minusdm_2  midprice_0  \\\n",
            "21449   78.518883   78.284455   9.841488  15.992612  24.952406    76.28500   \n",
            "1153     5.764215    5.687515   0.458189   1.183627   2.595210     6.07750   \n",
            "1808    12.652382   12.448386   0.283356   0.915470   2.386618    12.46500   \n",
            "12160   31.928411   31.351326   1.219086   3.302505   6.650203    31.97500   \n",
            "18531   33.288377   33.294406   0.486469   1.895135   4.900130    34.13000   \n",
            "7496    80.085156   78.485376   4.204412   9.638322  19.460071    83.43495   \n",
            "21265   66.295450   67.491457   3.252103  10.195739  20.500605    64.68500   \n",
            "2812    41.463673   41.252964   1.785104   4.973304   8.931452    40.95665   \n",
            "18935   43.278067   43.319667   3.496496   6.244536  10.470780    43.10500   \n",
            "21994  116.678586  116.859319   2.947403  10.668776  22.804614   117.25000   \n",
            "\n",
            "       midprice_1  midprice_2  aroon_0  aroon_1  aroon_2     trima_0  \\\n",
            "21449    76.79000    76.79000    -20.0    -56.0    -28.0   77.551000   \n",
            "1153      5.69250     5.59500     50.0     80.0     42.0    6.098000   \n",
            "1808     10.42500     9.92500     80.0     80.0     94.0   12.636000   \n",
            "12160    32.32375    32.23750     20.0    -44.0    -12.0   31.882333   \n",
            "18531    33.42500    33.35000    100.0     96.0    100.0   33.876667   \n",
            "7496     81.66495    79.75995    -80.0     36.0     60.0   82.924333   \n",
            "21265    65.28500    67.92500     70.0    -28.0    -86.0   64.082000   \n",
            "2812     41.23550    41.50720    -60.0     -8.0    -34.0   40.555447   \n",
            "18935    43.10500    43.10500    -10.0     -4.0     -2.0   43.619000   \n",
            "21994   117.25000   117.43000     80.0     32.0    -40.0  117.768333   \n",
            "\n",
            "          trima_1     trima_2      rsi_0      rsi_1      rsi_2       dx_0  \\\n",
            "21449   79.006982   78.863569  37.971448  41.846120  45.327632  29.363154   \n",
            "1153     5.912544    5.665492  48.440277  52.476903  52.068510  16.524354   \n",
            "1808    11.686509    8.996985  74.606235  75.788822  71.168961  69.367081   \n",
            "12160   32.324549   32.187221  46.342590  49.506580  52.224475  27.006745   \n",
            "18531   33.269882   33.207185  74.954558  62.183900  56.148219  52.025535   \n",
            "7496    81.105917   78.925108  55.440244  56.199783  54.903554  10.070503   \n",
            "21265   65.306213   67.473985  59.764029  48.486122  46.653711  15.161139   \n",
            "2812    40.825207   41.448188  47.029927  48.662453  51.491641   2.994945   \n",
            "18935   43.727219   43.877708  34.073962  41.794972  46.193949  36.330918   \n",
            "21994  115.860059  117.362215  75.061453  61.343998  57.259201  44.156798   \n",
            "\n",
            "            dx_1       dx_2      mfi_0      mfi_1      mfi_2  minusdi_0  \\\n",
            "21449  19.537259  11.637454  44.655043  47.315170  44.783632  47.183332   \n",
            "1153    3.106452   0.460401  44.400598  63.253641  57.252344  22.707354   \n",
            "1808   62.981269  47.168554  76.011610  78.664021  76.873486   6.237891   \n",
            "12160  16.152864   6.505479  52.275617  51.104442  53.051869  29.886138   \n",
            "18531  27.559919  13.488901  72.729518  63.380112  66.220514   9.786487   \n",
            "7496    4.118703   3.080946  46.337019  53.047293  54.820040  25.332562   \n",
            "21265   9.301369  13.240458  43.143152  53.690287  44.333453  26.068779   \n",
            "2812    1.163113   4.059857  39.560741  43.577911  51.418373  21.430542   \n",
            "18935  22.434787  13.046898  35.948300  35.097272  41.390229  35.190979   \n",
            "21994  18.698320  10.505996  70.255808  51.716924  50.137557  19.409008   \n",
            "\n",
            "       minusdi_1  minusdi_2    rocr_0    rocr_1    rocr_2       ema_0  \\\n",
            "21449  40.261790  35.039735  0.970306  0.947402  0.930804   76.301844   \n",
            "1153   22.023114  21.823654  0.953376  1.162745  1.076225    6.025951   \n",
            "1808    8.558426  12.573539  1.074380  1.653944  1.908957   12.547552   \n",
            "12160  27.219913  24.885946  0.998513  0.990992  1.004013   31.931964   \n",
            "18531  14.887920  18.319615  1.029368  1.071980  1.083698   34.011950   \n",
            "7496   23.163104  23.014107  0.983771  1.043941  1.061687   82.272532   \n",
            "21265  31.436104  31.998234  1.024274  0.988289  0.920475   65.007328   \n",
            "2812   23.612455  22.485009  0.979759  0.948099  0.999825   40.535773   \n",
            "18935  29.561286  26.237870  0.968423  0.942812  0.968199   42.861095   \n",
            "21994  26.587247  27.821204  1.071078  1.046229  1.008500  118.648768   \n",
            "\n",
            "            ema_1       ema_2        t3_0        t3_1        t3_2   mom_0  \\\n",
            "21449   77.683199   78.208824   77.405906   79.084190   79.182777 -2.2600   \n",
            "1153     5.912502    5.797497    6.149228    5.768779    5.673636 -0.2900   \n",
            "1808    11.422531   10.035662   13.036249   10.731563    8.027672  0.9000   \n",
            "12160   32.068773   31.955806   31.977276   32.407676   32.662777 -0.0475   \n",
            "18531   33.591316   33.298420   33.748785   33.341315   32.837309  0.9900   \n",
            "7496    81.239863   79.875245   83.186753   80.460731   78.418805 -1.3600   \n",
            "21265   65.674409   67.273374   64.028819   65.290245   69.329574  1.5800   \n",
            "2812    40.825397   40.796372   40.694031   41.410386   42.262166 -0.8366   \n",
            "18935   43.382135   43.512396   43.556268   43.934604   43.932223 -1.3600   \n",
            "21994  117.484486  116.830361  116.422697  117.233125  118.285356  8.1100   \n",
            "\n",
            "        mom_1   mom_2     adxr_0     adxr_1     adxr_2       cci_0  \\\n",
            "21449 -4.1000 -5.4900  30.236727   8.949177   5.411546  -88.721456   \n",
            "1153   0.8300  0.4200  20.224251   9.540382   5.842892 -173.883162   \n",
            "1808   5.1400  6.1900  66.964083  30.055823  18.587310  111.184031   \n",
            "12160 -0.2900  0.1275  23.868198   9.356841   9.528970   -0.533523   \n",
            "18531  2.3300  2.6800  22.741630  10.907966   6.891581  135.314861   \n",
            "7496   3.4700  4.7900  23.532148  11.247103   6.309996  -84.169266   \n",
            "21265 -0.7900 -5.7600  27.003972  20.483873  10.236297  195.155929   \n",
            "2812  -2.2168 -0.0071  14.461836  17.112964  19.885144  -26.524107   \n",
            "18935 -2.5300 -1.3700  20.526629   7.968811   5.749695  -94.003786   \n",
            "21994  5.4000  1.0300  20.938379   9.437489   7.822690  133.536279   \n",
            "\n",
            "            cci_1       cci_2      tema_0      tema_1      tema_2    natr_0  \\\n",
            "21449 -145.663934 -186.129836   73.909051   75.621462   77.072587  2.824371   \n",
            "1153    23.836836   49.938603    6.005880    6.142825    6.036023  3.402698   \n",
            "1808    78.011352  108.461781   12.920207   13.538190   13.176373  3.494223   \n",
            "12160  -76.080146  -33.108824   31.752128   31.820318   32.050617  1.278616   \n",
            "18531  174.542108  189.464146   34.604001   34.217876   33.967783  1.432515   \n",
            "7496    31.109801   83.265947   82.161261   83.292927   82.773131  2.013206   \n",
            "21265   81.898533  -30.440099   65.716093   64.298211   63.498540  1.871170   \n",
            "2812   -32.142804  -71.460387   40.223278   40.222888   40.623008  2.056955   \n",
            "18935 -201.658458 -268.588923   41.730199   42.641332   43.207045  2.382109   \n",
            "21994  175.733077  167.874125  121.654806  118.671524  117.912901  1.242595   \n",
            "\n",
            "         natr_1    natr_2      sma_0       sma_1       sma_2   plusdi_0  \\\n",
            "21449  2.151473  1.928551   76.61500   78.784800   78.630800  25.763764   \n",
            "1153   3.625279  4.010702    6.08300    5.844400    5.767300  16.267081   \n",
            "1808   3.291295  2.920196   12.60600   11.315200    9.337800  34.488824   \n",
            "12160  1.521220  1.675275   31.89925   32.303700   32.092775  17.176147   \n",
            "18531  1.467360  1.541678   33.92600   33.365600   33.277800  31.012246   \n",
            "7496   2.018955  2.051362   83.00400   81.117400   79.025200  20.697139   \n",
            "21265  1.945896  1.921942   64.28600   65.334800   67.810400  35.386028   \n",
            "2812   2.080454  1.961795   40.74352   40.789424   41.449880  22.753840   \n",
            "18935  2.025800  1.913552   43.33600   43.608400   43.684200  16.434844   \n",
            "21994  1.313392  1.341436  117.75300  116.898800  117.301800  50.103510   \n",
            "\n",
            "        plusdi_1   plusdi_2   midpoint         adosc            ad  trange  \\\n",
            "21449  27.100956  27.734421   72.98000 -7.703896e+05  9.117359e+07  1.9800   \n",
            "1153   20.696063  21.623622    6.02000 -7.647607e+05  2.766290e+07  0.1650   \n",
            "1808   37.679929  35.025155   12.85500  1.686494e+06  7.291043e+07  0.4000   \n",
            "12160  19.649207  21.845820   31.81000 -1.184604e+07 -2.670017e+07  0.3650   \n",
            "18531  26.216175  24.032442   34.52500 -5.546956e+05 -8.678138e+07  0.5800   \n",
            "7496   25.153105  24.477291   81.99750 -1.462621e+07 -1.833596e+08  2.0100   \n",
            "21265  26.085782  24.515550   65.41500  8.225269e+05  7.971495e+07  1.0000   \n",
            "2812   23.069491  24.387986   40.25585 -4.040141e+05  2.801138e+07  0.6936   \n",
            "18935  18.727745  20.181572   41.60000  7.799613e+06  5.281170e+07  0.5799   \n",
            "21994  38.816683  34.353250  120.75000  7.573293e+05  1.470866e+08  1.4800   \n",
            "\n",
            "              sar  httrendline      macd  htdcperiod  httrendmode   htdcphase  \\\n",
            "21449   80.984200    78.732625 -0.584608   27.878289          1.0  117.532387   \n",
            "1153     5.684693     5.827477 -0.016032   25.598961          1.0   56.350467   \n",
            "1808    12.554000    11.124347 -0.073380   25.266330          1.0  121.879553   \n",
            "12160   31.277479    32.319379 -0.061569   24.314455          1.0  192.124264   \n",
            "18531   32.731531    33.423542  0.133981   18.985250          1.0   36.643481   \n",
            "7496    85.966422    81.153938 -0.179569   21.176396          0.0  190.641565   \n",
            "21265   67.196507    64.970835  0.360271   15.858680          0.0  175.513851   \n",
            "2812    42.441238    40.792430 -0.059495   15.601760          0.0  205.282477   \n",
            "18935   45.155027    43.566652 -0.277513   16.673528          1.0  212.760211   \n",
            "21994  112.889923   116.563993  0.793122   22.513219          1.0  -19.356004   \n",
            "\n",
            "                obv     ultosc  NasdaqComposite_Close  NasdaqComposite_Volume  \\\n",
            "21449  1.694593e+07  39.036533            4691.870117            2.074090e+09   \n",
            "1153   5.937199e+07  40.177995            5159.740234            1.909470e+09   \n",
            "1808   1.503480e+08  60.070678            7643.410156            2.366350e+09   \n",
            "12160  5.685134e+08  49.661894            5153.970215            1.619970e+09   \n",
            "18531  4.673670e+08  55.770616            3919.919922            1.776860e+09   \n",
            "7496   3.386973e+09  46.062023            4688.540039            1.739950e+09   \n",
            "21265  5.935012e+06  58.274471            4781.259766            2.005320e+09   \n",
            "2812   4.201064e+07  46.592983            4307.600098            1.992750e+09   \n",
            "18935  5.099249e+08  43.031486            5160.089844            1.613540e+09   \n",
            "21994  8.280912e+07  71.260886            8017.899902            2.319130e+09   \n",
            "\n",
            "       NasdaqComposite_%change_after_10  NasdaqComposite_%change_after_25  \\\n",
            "21449                         -0.031316                         -0.015508   \n",
            "1153                           0.013715                          0.099719   \n",
            "1808                           0.011768                          0.032603   \n",
            "12160                          0.026354                          0.020934   \n",
            "18531                         -0.008203                          0.060920   \n",
            "7496                           0.012482                          0.096416   \n",
            "21265                         -0.009876                         -0.009747   \n",
            "2812                          -0.011558                          0.027809   \n",
            "18935                          0.029163                          0.016078   \n",
            "21994                          0.025345                          0.022447   \n",
            "\n",
            "       NasdaqComposite_%change_after_50  Russell2000_Close  \\\n",
            "21449                         -0.054062        1107.300049   \n",
            "1153                           0.082664        1212.729980   \n",
            "1808                           0.160815        1555.880005   \n",
            "12160                          0.031623        1292.390015   \n",
            "18531                          0.085066        1101.469971   \n",
            "7496                           0.023051        1173.810059   \n",
            "21265                         -0.060403        1141.640015   \n",
            "2812                           0.047140        1195.660034   \n",
            "18935                          0.034449        1295.800049   \n",
            "21994                          0.035051        1728.410034   \n",
            "\n",
            "       Russell2000_Volume  Russell2000_%change_after_10  \\\n",
            "21449        4.385810e+09                     -0.035302   \n",
            "1153         3.786530e+09                      0.002472   \n",
            "1808         3.766150e+09                     -0.015920   \n",
            "12160        3.030020e+09                      0.030861   \n",
            "18531        3.221030e+09                     -0.017483   \n",
            "7496         3.227130e+09                      0.000256   \n",
            "21265        4.334490e+09                     -0.016751   \n",
            "2812         3.289210e+09                     -0.008500   \n",
            "18935        3.091190e+09                      0.036922   \n",
            "21994        2.854080e+09                      0.031690   \n",
            "\n",
            "       Russell2000_%change_after_25  Russell2000_%change_after_50  \\\n",
            "21449                     -0.003662                     -0.028070   \n",
            "1153                       0.095214                      0.091203   \n",
            "1808                       0.023450                      0.162910   \n",
            "12160                      0.038941                      0.021838   \n",
            "18531                      0.051764                      0.083847   \n",
            "7496                       0.114391                      0.003145   \n",
            "21265                     -0.018290                     -0.068802   \n",
            "2812                       0.058893                      0.042278   \n",
            "18935                      0.030441                      0.023870   \n",
            "21994                      0.017664                      0.026427   \n",
            "\n",
            "       SP500_Close  SP500_%change_after_10  SP500_%change_after_25  \\\n",
            "21449      2036.09               -0.018903               -0.005835   \n",
            "1153       2163.79               -0.004248                0.062718   \n",
            "1808       2810.92                0.006639                0.026745   \n",
            "12160      2122.85                0.020954                0.000057   \n",
            "18531      1767.69               -0.002404                0.067800   \n",
            "7496       2039.82                0.010788                0.070137   \n",
            "21265      1987.05                0.010209               -0.000915   \n",
            "2812       1860.77               -0.006959                0.022542   \n",
            "18935      2124.20                0.021176               -0.002348   \n",
            "21994      2896.74                0.026510                0.031977   \n",
            "\n",
            "       SP500_%change_after_50  DowJones_Close  DowJones_Volume  \\\n",
            "21449               -0.027813      17409.7207     4.385810e+09   \n",
            "1153                 0.056517      18355.0000     3.786530e+09   \n",
            "1808                 0.130818      25702.8906     3.766150e+09   \n",
            "12160                0.009890      18119.7793     3.030020e+09   \n",
            "18531                0.078011      15750.6699     3.221030e+09   \n",
            "7496                 0.015993      17634.7402     3.227130e+09   \n",
            "21265               -0.044527      16776.4297     4.334490e+09   \n",
            "2812                 0.018612      16222.1699     3.289210e+09   \n",
            "18935                0.015183      18144.0703     3.091190e+09   \n",
            "21994                0.042120      26049.6406     2.854080e+09   \n",
            "\n",
            "       DowJones_%change_after_10  DowJones_%change_after_25  \\\n",
            "21449                  -0.014999                  -0.004757   \n",
            "1153                   -0.012908                   0.054296   \n",
            "1808                   -0.010863                   0.011466   \n",
            "12160                   0.019882                  -0.008361   \n",
            "18531                   0.004485                   0.065925   \n",
            "7496                    0.014043                   0.065923   \n",
            "21265                   0.016126                   0.008017   \n",
            "2812                   -0.008436                   0.014217   \n",
            "18935                   0.021393                  -0.008460   \n",
            "21994                   0.034221                   0.040143   \n",
            "\n",
            "       DowJones_%change_after_50  Fair Buy  Fair Sell  Hold  Moderate Buy  \\\n",
            "21449                  -0.033017         0          0     0             0   \n",
            "1153                    0.049281         0          1     0             0   \n",
            "1808                    0.114493         0          0     1             0   \n",
            "12160                   0.003441         0          0     0             1   \n",
            "18531                   0.061798         0          1     0             0   \n",
            "7496                    0.029023         0          0     1             0   \n",
            "21265                  -0.045086         0          0     0             0   \n",
            "2812                   -0.012355         0          0     1             0   \n",
            "18935                   0.009291         1          0     0             0   \n",
            "21994                   0.038228         0          0     1             0   \n",
            "\n",
            "       Moderate Sell  Strong Buy  Strong Sell  \n",
            "21449              1           0            0  \n",
            "1153               0           0            0  \n",
            "1808               0           0            0  \n",
            "12160              0           0            0  \n",
            "18531              0           0            0  \n",
            "7496               0           0            0  \n",
            "21265              1           0            0  \n",
            "2812               0           0            0  \n",
            "18935              0           0            0  \n",
            "21994              0           0            0   \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ter1ICUeEtes",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0c5a9bc-2cd6-4dd0-d663-dac1eb247427"
      },
      "source": [
        "# Deal with missing data\n",
        "null_data = data[data.isnull().any(axis=1)]\n",
        "num_nan_rows = len(null_data)\n",
        "print('\\n')\n",
        "print(num_nan_rows, 'rows with NaN in at least one column found')\n",
        "if num_nan_rows != 0:\n",
        "    if num_nan_rows >= 5:\n",
        "        full_print(null_data.sample(5))\n",
        "    else:\n",
        "        full_print(null_data)\n",
        "\n",
        "    # No-crash user prompt for dealing with missing data by dropping the rows or by filling with relative mean\n",
        "    while True:\n",
        "        # print('--------------------\\nA) Drop the rows\\nB) Fill with mean')\n",
        "        # letter_choice = input('Enter your option\\'s letter choice for dealing with the missing data: ').lower()\n",
        "        # if letter_choice == 'a':\n",
        "        #     data.dropna(inplace=True)\n",
        "        # elif letter_choice == 'b':\n",
        "        #     data.fillna(value=data.mean(), inplace=True)\n",
        "        # else:\n",
        "        #     print('Invalid choice, try again.')\n",
        "        #     continue\n",
        "        data.fillna(value=data.mean(), inplace=True)\n",
        "        break"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "64 rows with NaN in at least one column found\n",
            "      future_%_change      volume   plusdm_0   plusdm_1   plusdm_2    trix_0  \\\n",
            "9029              NaN         NaN        NaN        NaN        NaN       NaN   \n",
            "8999              NaN         NaN        NaN        NaN        NaN       NaN   \n",
            "9021              NaN         NaN        NaN        NaN        NaN       NaN   \n",
            "8995              NaN  18754853.0  24.771571  49.090562  94.487993  0.198145   \n",
            "8981              NaN  26619517.0  10.570487  35.033928  82.639461 -0.090623   \n",
            "\n",
            "        trix_1    trix_2      adx_0     adx_1    adx_2       wma_0  \\\n",
            "9029       NaN       NaN        NaN       NaN      NaN         NaN   \n",
            "8999       NaN       NaN        NaN       NaN      NaN         NaN   \n",
            "9021       NaN       NaN        NaN       NaN      NaN         NaN   \n",
            "8995 -0.034947 -0.002751  26.850870  9.260425  5.66965  274.623636   \n",
            "8981 -0.041787  0.017974  14.460624  7.795647  5.85042  262.370273   \n",
            "\n",
            "           wma_1       wma_2  aroonosc_0  aroonosc_1  aroonosc_2     atr_0  \\\n",
            "9029         NaN         NaN         NaN         NaN         NaN       NaN   \n",
            "8999         NaN         NaN         NaN         NaN         NaN       NaN   \n",
            "9021         NaN         NaN         NaN         NaN         NaN       NaN   \n",
            "8995  267.894338  267.047008        90.0        36.0        18.0  9.112135   \n",
            "8981  265.833538  266.175878       -70.0       -76.0        18.0  8.375978   \n",
            "\n",
            "         atr_1     atr_2    willr_0    willr_1    willr_2      dema_0  \\\n",
            "9029       NaN       NaN        NaN        NaN        NaN         NaN   \n",
            "8999       NaN       NaN        NaN        NaN        NaN         NaN   \n",
            "9021       NaN       NaN        NaN        NaN        NaN         NaN   \n",
            "8995  8.707808  8.561567 -24.406671 -23.250840 -19.369369  278.533982   \n",
            "8981  8.163157  8.277252 -84.131206 -89.068702 -69.155998  259.727757   \n",
            "\n",
            "          dema_1      dema_2     roc_0     roc_1     roc_2      kama_0  \\\n",
            "9029         NaN         NaN       NaN       NaN       NaN         NaN   \n",
            "8999         NaN         NaN       NaN       NaN       NaN         NaN   \n",
            "9021         NaN         NaN       NaN       NaN       NaN         NaN   \n",
            "8995  270.989737  267.849347  8.143145  2.482804  2.823191  268.991464   \n",
            "8981  263.424409  265.508436 -4.722808 -5.588742 -6.043255  265.480328   \n",
            "\n",
            "          kama_1      kama_2  minusdm_0  minusdm_1  minusdm_2  midprice_0  \\\n",
            "9029         NaN         NaN        NaN        NaN        NaN         NaN   \n",
            "8999         NaN         NaN        NaN        NaN        NaN         NaN   \n",
            "9021         NaN         NaN        NaN        NaN        NaN         NaN   \n",
            "8995  265.799875  266.849557   7.384008  30.099243  75.950562      270.64   \n",
            "8981  265.917053  266.958896  15.455571  42.860178  91.723318      265.32   \n",
            "\n",
            "      midprice_1  midprice_2  aroon_0  aroon_1  aroon_2     trima_0  \\\n",
            "9029         NaN         NaN      NaN      NaN      NaN         NaN   \n",
            "8999         NaN         NaN      NaN      NaN      NaN         NaN   \n",
            "9021         NaN         NaN      NaN      NaN      NaN         NaN   \n",
            "8995     269.865       265.7     90.0     36.0     18.0  270.753333   \n",
            "8981     270.415       265.7    -70.0    -76.0     18.0  264.983667   \n",
            "\n",
            "         trima_1     trima_2      rsi_0      rsi_1      rsi_2       dx_0  \\\n",
            "9029         NaN         NaN        NaN        NaN        NaN        NaN   \n",
            "8999         NaN         NaN        NaN        NaN        NaN        NaN   \n",
            "9021         NaN         NaN        NaN        NaN        NaN        NaN   \n",
            "8995  263.213018  266.186046  59.593835  54.686059  52.641513  54.073239   \n",
            "8981  268.063254  266.573677  39.088732  44.798767  47.870914  18.769971   \n",
            "\n",
            "           dx_1       dx_2      mfi_0      mfi_1      mfi_2  minusdi_0  \\\n",
            "9029        NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "8999        NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "9021        NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "8995  23.982025  10.876313  71.755838  58.331415  53.598251   8.103488   \n",
            "8981  10.047294   5.209746  44.470833  49.172415  46.227549  18.452259   \n",
            "\n",
            "      minusdi_1  minusdi_2    rocr_0    rocr_1    rocr_2       ema_0  \\\n",
            "9029        NaN        NaN       NaN       NaN       NaN         NaN   \n",
            "8999        NaN        NaN       NaN       NaN       NaN         NaN   \n",
            "9021        NaN        NaN       NaN       NaN       NaN         NaN   \n",
            "8995  13.826323  17.742211  1.081431  1.024828  1.028232  272.849424   \n",
            "8981  21.001767  22.162747  0.952772  0.944113  0.939567  263.099478   \n",
            "\n",
            "           ema_1       ema_2        t3_0        t3_1        t3_2  mom_0  \\\n",
            "9029         NaN         NaN         NaN         NaN         NaN    NaN   \n",
            "8999         NaN         NaN         NaN         NaN         NaN    NaN   \n",
            "9021         NaN         NaN         NaN         NaN         NaN    NaN   \n",
            "8995  268.616542  267.991589  266.659823  263.716151  269.090993  20.98   \n",
            "8981  265.727720  267.418461  267.091169  265.705069  272.898404 -12.77   \n",
            "\n",
            "      mom_1  mom_2     adxr_0    adxr_1    adxr_2      cci_0       cci_1  \\\n",
            "9029    NaN    NaN        NaN       NaN       NaN        NaN         NaN   \n",
            "8999    NaN    NaN        NaN       NaN       NaN        NaN         NaN   \n",
            "9021    NaN    NaN        NaN       NaN       NaN        NaN         NaN   \n",
            "8995   6.75   7.65  19.696419  8.915326  6.637769  97.945239  153.288564   \n",
            "8981 -15.25 -16.57  13.939128  8.948913  7.606531 -77.303994 -110.794270   \n",
            "\n",
            "           cci_2      tema_0      tema_1      tema_2    natr_0    natr_1  \\\n",
            "9029         NaN         NaN         NaN         NaN       NaN       NaN   \n",
            "8999         NaN         NaN         NaN         NaN       NaN       NaN   \n",
            "9021         NaN         NaN         NaN         NaN       NaN       NaN   \n",
            "8995  158.010091  281.862033  274.484564  267.887935  3.270453  3.125335   \n",
            "8981  -88.833376  257.448164  262.472463  262.417783  3.251292  3.168681   \n",
            "\n",
            "        natr_2     sma_0     sma_1     sma_2   plusdi_0   plusdi_1   plusdi_2  \\\n",
            "9029       NaN       NaN       NaN       NaN        NaN        NaN        NaN   \n",
            "8999       NaN       NaN       NaN       NaN        NaN        NaN        NaN   \n",
            "9021       NaN       NaN       NaN       NaN        NaN        NaN        NaN   \n",
            "8995  3.072847  270.8170  266.6926  265.8643  27.185255  22.550134  22.072593   \n",
            "8981  3.212969  265.1795  267.6178  267.2859  12.620004  17.166853  19.967850   \n",
            "\n",
            "      midpoint         adosc            ad   trange         sar  httrendline  \\\n",
            "9029       NaN           NaN           NaN      NaN         NaN          NaN   \n",
            "8999       NaN           NaN           NaN      NaN         NaN          NaN   \n",
            "9021       NaN           NaN           NaN      NaN         NaN          NaN   \n",
            "8995   281.315  6.071418e+06  1.604554e+09   6.9099  255.726420   265.017496   \n",
            "8981   259.500 -5.726806e+06  1.601442e+09  10.8600  282.909686   267.153357   \n",
            "\n",
            "          macd  htdcperiod  httrendmode  htdcphase           obv     ultosc  \\\n",
            "9029       NaN         NaN          NaN        NaN           NaN        NaN   \n",
            "8999       NaN         NaN          NaN        NaN           NaN        NaN   \n",
            "9021       NaN         NaN          NaN        NaN           NaN        NaN   \n",
            "8995  2.408589   18.363121          1.0        0.0  5.333085e+09  53.517597   \n",
            "8981 -1.337114   14.103345          1.0        0.0  5.250660e+09  44.023145   \n",
            "\n",
            "      NasdaqComposite_Close  NasdaqComposite_Volume  \\\n",
            "9029           12764.750000            4.994090e+09   \n",
            "8999           11590.780273            3.606820e+09   \n",
            "9021           12519.950195            4.741690e+09   \n",
            "8995           11185.589844            3.222460e+09   \n",
            "8981           11579.940430            3.475620e+09   \n",
            "\n",
            "      NasdaqComposite_%change_after_10  NasdaqComposite_%change_after_25  \\\n",
            "9029                          0.031313                          0.083004   \n",
            "8999                          0.009238                          0.037902   \n",
            "9021                          0.056093                          0.147399   \n",
            "8995                         -0.045099                          0.048099   \n",
            "8981                          0.061060                          0.010634   \n",
            "\n",
            "      NasdaqComposite_%change_after_50  Russell2000_Close  Russell2000_Volume  \\\n",
            "9029                          0.123203        1978.050049        4.184930e+09   \n",
            "8999                          0.010841        1615.079956        4.783040e+09   \n",
            "9021                          0.147192        1891.250000        4.788560e+09   \n",
            "8995                          0.003511        1561.579956        4.903070e+09   \n",
            "8981                          0.093705        1637.550049        3.939060e+09   \n",
            "\n",
            "      Russell2000_%change_after_10  Russell2000_%change_after_25  \\\n",
            "9029                      0.069968                      0.138813   \n",
            "8999                      0.007046                      0.071228   \n",
            "9021                      0.059322                      0.229298   \n",
            "8995                     -0.047166                      0.075602   \n",
            "8981                      0.110271                      0.060122   \n",
            "\n",
            "      Russell2000_%change_after_50  SP500_Close  SP500_%change_after_10  \\\n",
            "9029                      0.227809      3722.48                0.015207   \n",
            "8999                      0.027921      3443.44                0.002294   \n",
            "9021                      0.282282      3691.96                0.037785   \n",
            "8995                     -0.006673      3310.11               -0.049731   \n",
            "8981                      0.095278      3477.13                0.054168   \n",
            "\n",
            "      SP500_%change_after_25  SP500_%change_after_50  DowJones_Close  \\\n",
            "9029                0.041935                0.088620      30303.3691   \n",
            "8999                0.023919               -0.000052      27847.6602   \n",
            "9021                0.129054                0.119298      30069.7891   \n",
            "8995                0.019565               -0.019183      26659.1094   \n",
            "8981                0.006388                0.071132      28586.9004   \n",
            "\n",
            "      DowJones_Volume  DowJones_%change_after_10  DowJones_%change_after_25  \\\n",
            "9029     4.184930e+09                   0.011140                   0.030810   \n",
            "8999     4.783040e+09                  -0.012873                   0.002374   \n",
            "9021     4.788560e+09                   0.027553                   0.134641   \n",
            "8995     4.903070e+09                  -0.064402                  -0.005830   \n",
            "8981     3.939060e+09                   0.051996                   0.010397   \n",
            "\n",
            "      DowJones_%change_after_50  Fair Buy  Fair Sell  Hold  Moderate Buy  \\\n",
            "9029                   0.070659         0          0     1             0   \n",
            "8999                  -0.014188         0          0     1             0   \n",
            "9021                   0.106566         0          0     1             0   \n",
            "8995                  -0.037330         0          0     1             0   \n",
            "8981                   0.086391         0          0     1             0   \n",
            "\n",
            "      Moderate Sell  Strong Buy  Strong Sell  \n",
            "9029              0           0            0  \n",
            "8999              0           0            0  \n",
            "9021              0           0            0  \n",
            "8995              0           0            0  \n",
            "8981              0           0            0   \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcscNOHFrNyR"
      },
      "source": [
        "# Save the its-not-much-but-its-honest-work-obtained giant dataframe as a csv\n",
        "from google.colab import files      # NOTE: This method is NOT scalable for larger datasets\n",
        "data.to_csv('samuel_900_data.csv')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEZ_RXPfBLpt"
      },
      "source": [
        "# Model Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIZU73ReBQBH"
      },
      "source": [
        "# Load data from notebook's virtual machine file directory\n",
        "data = pd.read_csv('samuel_900_data.csv')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Gc1mMEE5YFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61887fc0-9b89-4d5a-c57d-0de691a3afd3"
      },
      "source": [
        "# Split the data into train and test data\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(data[input_columns], data[output_columns], test_size=0.25, random_state=42069, shuffle=True)\n",
        "print('Number of examples in training data:', len(train_data))\n",
        "print('Number of examples in testing data:', len(test_data))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of examples in training data: 18627\n",
            "Number of examples in testing data: 6210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iahrga0doYkC"
      },
      "source": [
        "# In-place apply standardization to input features of training and test sets\n",
        "scaler = StandardScaler().fit(train_data)      # NOTE: Use for transforming new data from user-interface loop\n",
        "train_data = scaler.transform(train_data)\n",
        "test_data = scaler.transform(test_data)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYwXj8va4dAO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619879bd-f8e3-4689-a2fe-cc800f67d13a"
      },
      "source": [
        "# Make the actual Keras model\n",
        "samuel_900 = keras.models.Sequential(name='SAMUEL_900', layers=[    # weird note, model names in keras can't have spaces\n",
        "    layers.InputLayer(input_shape=[len(input_columns)], name='input_layer'),       # input layer\n",
        "    layers.Dense(116, activation='relu', name='hidden_layer_1'),\n",
        "    layers.Dense(100, activation='relu', name='hidden_layer_2'),\n",
        "    layers.Dense(80, activation='relu', name='hidden_layer_3'),\n",
        "    layers.Dense(80, activation='relu', name='hidden_layer_4'),\n",
        "    layers.Dense(90, activation='relu', name='hidden_layer_5'),\n",
        "    layers.Dense(50, activation='relu', name='hidden_layer_6'),\n",
        "    layers.Dense(len(output_columns), activation='softmax', name='output_layer')    # output layer\n",
        "])\n",
        "samuel_900.summary()\n",
        "samuel_900.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])     # must use categorical_crossentropy for one-hot, sparse version is for binary"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"SAMUEL_900\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer_1 (Dense)       (None, 116)               13572     \n",
            "_________________________________________________________________\n",
            "hidden_layer_2 (Dense)       (None, 100)               11700     \n",
            "_________________________________________________________________\n",
            "hidden_layer_3 (Dense)       (None, 80)                8080      \n",
            "_________________________________________________________________\n",
            "hidden_layer_4 (Dense)       (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "hidden_layer_5 (Dense)       (None, 90)                7290      \n",
            "_________________________________________________________________\n",
            "hidden_layer_6 (Dense)       (None, 50)                4550      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 7)                 357       \n",
            "=================================================================\n",
            "Total params: 52,029\n",
            "Trainable params: 52,029\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuOsm-wbkuBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986665bb-919e-489c-861b-24f999ecaca0"
      },
      "source": [
        "# Fit the model and track its training history\n",
        "num_epochs = 200    # CONFIG\n",
        "training_batch_size = 64     # CONFIG (ideally powers of 2)\n",
        "history = samuel_900.fit(train_data, train_labels, batch_size=training_batch_size, epochs=num_epochs, shuffle=False, validation_data=(test_data, test_labels))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "292/292 [==============================] - 2s 5ms/step - loss: 1.6085 - accuracy: 0.3811 - val_loss: 1.4729 - val_accuracy: 0.4056\n",
            "Epoch 2/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 1.4297 - accuracy: 0.4171 - val_loss: 1.3459 - val_accuracy: 0.4618\n",
            "Epoch 3/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 1.2972 - accuracy: 0.4671 - val_loss: 1.2417 - val_accuracy: 0.4881\n",
            "Epoch 4/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 1.1585 - accuracy: 0.5224 - val_loss: 1.1765 - val_accuracy: 0.5177\n",
            "Epoch 5/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 1.0337 - accuracy: 0.5742 - val_loss: 1.1109 - val_accuracy: 0.5432\n",
            "Epoch 6/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.9404 - accuracy: 0.6121 - val_loss: 1.0747 - val_accuracy: 0.5588\n",
            "Epoch 7/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.8707 - accuracy: 0.6392 - val_loss: 1.0674 - val_accuracy: 0.5615\n",
            "Epoch 8/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.8101 - accuracy: 0.6674 - val_loss: 1.0403 - val_accuracy: 0.5729\n",
            "Epoch 9/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.7515 - accuracy: 0.6889 - val_loss: 1.0268 - val_accuracy: 0.5828\n",
            "Epoch 10/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.7097 - accuracy: 0.7113 - val_loss: 1.0532 - val_accuracy: 0.5845\n",
            "Epoch 11/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.6745 - accuracy: 0.7231 - val_loss: 1.0650 - val_accuracy: 0.5963\n",
            "Epoch 12/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.7354 - val_loss: 0.9952 - val_accuracy: 0.6158\n",
            "Epoch 13/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.6155 - accuracy: 0.7450 - val_loss: 0.9076 - val_accuracy: 0.6481\n",
            "Epoch 14/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.5862 - accuracy: 0.7615 - val_loss: 0.9154 - val_accuracy: 0.6501\n",
            "Epoch 15/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.5520 - accuracy: 0.7739 - val_loss: 0.9418 - val_accuracy: 0.6449\n",
            "Epoch 16/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.5349 - accuracy: 0.7798 - val_loss: 0.9415 - val_accuracy: 0.6459\n",
            "Epoch 17/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.5180 - accuracy: 0.7905 - val_loss: 0.9464 - val_accuracy: 0.6556\n",
            "Epoch 18/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.4894 - accuracy: 0.8018 - val_loss: 0.9565 - val_accuracy: 0.6654\n",
            "Epoch 19/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.4805 - accuracy: 0.8076 - val_loss: 0.9491 - val_accuracy: 0.6639\n",
            "Epoch 20/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.4716 - accuracy: 0.8104 - val_loss: 0.9522 - val_accuracy: 0.6744\n",
            "Epoch 21/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.4606 - accuracy: 0.8148 - val_loss: 0.9415 - val_accuracy: 0.6757\n",
            "Epoch 22/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.4391 - accuracy: 0.8225 - val_loss: 0.9491 - val_accuracy: 0.6696\n",
            "Epoch 23/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.4208 - accuracy: 0.8327 - val_loss: 0.9623 - val_accuracy: 0.6721\n",
            "Epoch 24/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.4021 - accuracy: 0.8397 - val_loss: 0.9856 - val_accuracy: 0.6717\n",
            "Epoch 25/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.4105 - accuracy: 0.8364 - val_loss: 0.9666 - val_accuracy: 0.6841\n",
            "Epoch 26/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3850 - accuracy: 0.8474 - val_loss: 0.9545 - val_accuracy: 0.6913\n",
            "Epoch 27/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3747 - accuracy: 0.8486 - val_loss: 0.9839 - val_accuracy: 0.6905\n",
            "Epoch 28/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3686 - accuracy: 0.8520 - val_loss: 0.9571 - val_accuracy: 0.6957\n",
            "Epoch 29/200\n",
            "292/292 [==============================] - 1s 3ms/step - loss: 0.3601 - accuracy: 0.8576 - val_loss: 0.9602 - val_accuracy: 0.6994\n",
            "Epoch 30/200\n",
            "292/292 [==============================] - 1s 3ms/step - loss: 0.3507 - accuracy: 0.8592 - val_loss: 1.0221 - val_accuracy: 0.6862\n",
            "Epoch 31/200\n",
            "292/292 [==============================] - 1s 3ms/step - loss: 0.3473 - accuracy: 0.8608 - val_loss: 1.0394 - val_accuracy: 0.6855\n",
            "Epoch 32/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3384 - accuracy: 0.8590 - val_loss: 1.0675 - val_accuracy: 0.6808\n",
            "Epoch 33/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3309 - accuracy: 0.8676 - val_loss: 1.0546 - val_accuracy: 0.6929\n",
            "Epoch 34/200\n",
            "292/292 [==============================] - 1s 3ms/step - loss: 0.3211 - accuracy: 0.8695 - val_loss: 1.0968 - val_accuracy: 0.6890\n",
            "Epoch 35/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3191 - accuracy: 0.8741 - val_loss: 1.0969 - val_accuracy: 0.6836\n",
            "Epoch 36/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3107 - accuracy: 0.8751 - val_loss: 1.1108 - val_accuracy: 0.6862\n",
            "Epoch 37/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3095 - accuracy: 0.8735 - val_loss: 1.0810 - val_accuracy: 0.6984\n",
            "Epoch 38/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2914 - accuracy: 0.8860 - val_loss: 1.0930 - val_accuracy: 0.6900\n",
            "Epoch 39/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3175 - accuracy: 0.8726 - val_loss: 1.1553 - val_accuracy: 0.6804\n",
            "Epoch 40/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.3118 - accuracy: 0.8734 - val_loss: 1.1716 - val_accuracy: 0.6845\n",
            "Epoch 41/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2859 - accuracy: 0.8878 - val_loss: 1.1620 - val_accuracy: 0.6939\n",
            "Epoch 42/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2937 - accuracy: 0.8859 - val_loss: 1.1442 - val_accuracy: 0.6953\n",
            "Epoch 43/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2831 - accuracy: 0.8829 - val_loss: 1.2050 - val_accuracy: 0.6852\n",
            "Epoch 44/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2773 - accuracy: 0.8899 - val_loss: 1.2429 - val_accuracy: 0.6910\n",
            "Epoch 45/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2680 - accuracy: 0.8904 - val_loss: 1.3161 - val_accuracy: 0.6897\n",
            "Epoch 46/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2649 - accuracy: 0.8911 - val_loss: 1.2524 - val_accuracy: 0.6963\n",
            "Epoch 47/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2668 - accuracy: 0.8923 - val_loss: 1.3009 - val_accuracy: 0.6948\n",
            "Epoch 48/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2674 - accuracy: 0.8898 - val_loss: 1.2944 - val_accuracy: 0.6863\n",
            "Epoch 49/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2837 - accuracy: 0.8915 - val_loss: 1.3080 - val_accuracy: 0.7019\n",
            "Epoch 50/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2486 - accuracy: 0.9036 - val_loss: 1.2709 - val_accuracy: 0.7024\n",
            "Epoch 51/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2457 - accuracy: 0.9006 - val_loss: 1.2865 - val_accuracy: 0.7002\n",
            "Epoch 52/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2522 - accuracy: 0.8995 - val_loss: 1.2401 - val_accuracy: 0.6963\n",
            "Epoch 53/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2387 - accuracy: 0.9062 - val_loss: 1.2457 - val_accuracy: 0.7042\n",
            "Epoch 54/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2368 - accuracy: 0.9053 - val_loss: 1.2618 - val_accuracy: 0.6979\n",
            "Epoch 55/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2283 - accuracy: 0.9097 - val_loss: 1.3540 - val_accuracy: 0.6910\n",
            "Epoch 56/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2379 - accuracy: 0.9076 - val_loss: 1.4166 - val_accuracy: 0.6931\n",
            "Epoch 57/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2326 - accuracy: 0.9082 - val_loss: 1.3230 - val_accuracy: 0.6982\n",
            "Epoch 58/200\n",
            "292/292 [==============================] - 1s 3ms/step - loss: 0.2297 - accuracy: 0.9113 - val_loss: 1.3106 - val_accuracy: 0.7011\n",
            "Epoch 59/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2294 - accuracy: 0.9114 - val_loss: 1.3188 - val_accuracy: 0.7042\n",
            "Epoch 60/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2220 - accuracy: 0.9128 - val_loss: 1.3138 - val_accuracy: 0.7077\n",
            "Epoch 61/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2250 - accuracy: 0.9111 - val_loss: 1.2982 - val_accuracy: 0.7064\n",
            "Epoch 62/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2017 - accuracy: 0.9223 - val_loss: 1.3958 - val_accuracy: 0.7056\n",
            "Epoch 63/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1978 - accuracy: 0.9248 - val_loss: 1.3489 - val_accuracy: 0.7060\n",
            "Epoch 64/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2097 - accuracy: 0.9214 - val_loss: 1.4239 - val_accuracy: 0.6990\n",
            "Epoch 65/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2001 - accuracy: 0.9211 - val_loss: 1.4704 - val_accuracy: 0.6939\n",
            "Epoch 66/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2444 - accuracy: 0.9051 - val_loss: 1.3637 - val_accuracy: 0.7032\n",
            "Epoch 67/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2084 - accuracy: 0.9203 - val_loss: 1.4057 - val_accuracy: 0.7060\n",
            "Epoch 68/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2121 - accuracy: 0.9170 - val_loss: 1.3680 - val_accuracy: 0.7008\n",
            "Epoch 69/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2040 - accuracy: 0.9209 - val_loss: 1.4519 - val_accuracy: 0.6982\n",
            "Epoch 70/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2045 - accuracy: 0.9235 - val_loss: 1.3695 - val_accuracy: 0.6961\n",
            "Epoch 71/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1955 - accuracy: 0.9233 - val_loss: 1.3373 - val_accuracy: 0.7034\n",
            "Epoch 72/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2019 - accuracy: 0.9208 - val_loss: 1.4779 - val_accuracy: 0.6890\n",
            "Epoch 73/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1925 - accuracy: 0.9262 - val_loss: 1.3675 - val_accuracy: 0.6994\n",
            "Epoch 74/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1857 - accuracy: 0.9243 - val_loss: 1.4545 - val_accuracy: 0.6989\n",
            "Epoch 75/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2101 - accuracy: 0.9196 - val_loss: 1.4194 - val_accuracy: 0.6995\n",
            "Epoch 76/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1970 - accuracy: 0.9241 - val_loss: 1.3916 - val_accuracy: 0.6944\n",
            "Epoch 77/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1812 - accuracy: 0.9298 - val_loss: 1.4675 - val_accuracy: 0.6862\n",
            "Epoch 78/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2036 - accuracy: 0.9235 - val_loss: 1.4051 - val_accuracy: 0.6965\n",
            "Epoch 79/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1913 - accuracy: 0.9290 - val_loss: 1.3436 - val_accuracy: 0.7045\n",
            "Epoch 80/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1845 - accuracy: 0.9259 - val_loss: 1.3671 - val_accuracy: 0.7000\n",
            "Epoch 81/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1690 - accuracy: 0.9352 - val_loss: 1.4481 - val_accuracy: 0.7026\n",
            "Epoch 82/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1739 - accuracy: 0.9327 - val_loss: 1.3749 - val_accuracy: 0.6981\n",
            "Epoch 83/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1856 - accuracy: 0.9272 - val_loss: 1.4109 - val_accuracy: 0.7034\n",
            "Epoch 84/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1790 - accuracy: 0.9307 - val_loss: 1.4186 - val_accuracy: 0.7040\n",
            "Epoch 85/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1780 - accuracy: 0.9332 - val_loss: 1.4165 - val_accuracy: 0.6947\n",
            "Epoch 86/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1874 - accuracy: 0.9301 - val_loss: 1.3946 - val_accuracy: 0.7069\n",
            "Epoch 87/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1817 - accuracy: 0.9312 - val_loss: 1.4310 - val_accuracy: 0.7066\n",
            "Epoch 88/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1627 - accuracy: 0.9376 - val_loss: 1.4947 - val_accuracy: 0.7008\n",
            "Epoch 89/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1967 - accuracy: 0.9259 - val_loss: 1.4293 - val_accuracy: 0.7113\n",
            "Epoch 90/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1633 - accuracy: 0.9371 - val_loss: 1.4682 - val_accuracy: 0.7021\n",
            "Epoch 91/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1750 - accuracy: 0.9345 - val_loss: 1.4549 - val_accuracy: 0.6977\n",
            "Epoch 92/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1611 - accuracy: 0.9374 - val_loss: 1.4922 - val_accuracy: 0.7010\n",
            "Epoch 93/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1618 - accuracy: 0.9397 - val_loss: 1.4229 - val_accuracy: 0.7116\n",
            "Epoch 94/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1686 - accuracy: 0.9368 - val_loss: 1.4923 - val_accuracy: 0.6931\n",
            "Epoch 95/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1586 - accuracy: 0.9392 - val_loss: 1.5051 - val_accuracy: 0.6937\n",
            "Epoch 96/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1724 - accuracy: 0.9369 - val_loss: 1.5223 - val_accuracy: 0.6939\n",
            "Epoch 97/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1658 - accuracy: 0.9394 - val_loss: 1.5175 - val_accuracy: 0.6990\n",
            "Epoch 98/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1618 - accuracy: 0.9391 - val_loss: 1.5535 - val_accuracy: 0.6974\n",
            "Epoch 99/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1441 - accuracy: 0.9465 - val_loss: 1.5670 - val_accuracy: 0.6984\n",
            "Epoch 100/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.2027 - accuracy: 0.9256 - val_loss: 1.5424 - val_accuracy: 0.7050\n",
            "Epoch 101/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1597 - accuracy: 0.9390 - val_loss: 1.4918 - val_accuracy: 0.7081\n",
            "Epoch 102/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1710 - accuracy: 0.9365 - val_loss: 1.4616 - val_accuracy: 0.7142\n",
            "Epoch 103/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1628 - accuracy: 0.9364 - val_loss: 1.4074 - val_accuracy: 0.7163\n",
            "Epoch 104/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1451 - accuracy: 0.9454 - val_loss: 1.4781 - val_accuracy: 0.7064\n",
            "Epoch 105/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1541 - accuracy: 0.9445 - val_loss: 1.5272 - val_accuracy: 0.7045\n",
            "Epoch 106/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1470 - accuracy: 0.9433 - val_loss: 1.5476 - val_accuracy: 0.6960\n",
            "Epoch 107/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1599 - accuracy: 0.9394 - val_loss: 1.5277 - val_accuracy: 0.7050\n",
            "Epoch 108/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1562 - accuracy: 0.9426 - val_loss: 1.4552 - val_accuracy: 0.6961\n",
            "Epoch 109/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1676 - accuracy: 0.9400 - val_loss: 1.4766 - val_accuracy: 0.6936\n",
            "Epoch 110/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1540 - accuracy: 0.9416 - val_loss: 1.4933 - val_accuracy: 0.7045\n",
            "Epoch 111/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1386 - accuracy: 0.9487 - val_loss: 1.5227 - val_accuracy: 0.6986\n",
            "Epoch 112/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1523 - accuracy: 0.9430 - val_loss: 1.5254 - val_accuracy: 0.7052\n",
            "Epoch 113/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.9471 - val_loss: 1.5725 - val_accuracy: 0.7071\n",
            "Epoch 114/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1600 - accuracy: 0.9415 - val_loss: 1.5554 - val_accuracy: 0.7090\n",
            "Epoch 115/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1413 - accuracy: 0.9463 - val_loss: 1.4994 - val_accuracy: 0.7013\n",
            "Epoch 116/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1560 - accuracy: 0.9427 - val_loss: 1.5502 - val_accuracy: 0.6992\n",
            "Epoch 117/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1550 - accuracy: 0.9418 - val_loss: 1.4888 - val_accuracy: 0.7079\n",
            "Epoch 118/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1440 - accuracy: 0.9483 - val_loss: 1.5420 - val_accuracy: 0.7024\n",
            "Epoch 119/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1462 - accuracy: 0.9435 - val_loss: 1.4854 - val_accuracy: 0.7048\n",
            "Epoch 120/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1328 - accuracy: 0.9500 - val_loss: 1.5539 - val_accuracy: 0.6984\n",
            "Epoch 121/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1485 - accuracy: 0.9442 - val_loss: 1.5039 - val_accuracy: 0.7056\n",
            "Epoch 122/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1523 - accuracy: 0.9428 - val_loss: 1.5033 - val_accuracy: 0.7069\n",
            "Epoch 123/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1180 - accuracy: 0.9558 - val_loss: 1.5565 - val_accuracy: 0.6976\n",
            "Epoch 124/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1362 - accuracy: 0.9498 - val_loss: 1.5828 - val_accuracy: 0.7042\n",
            "Epoch 125/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1526 - accuracy: 0.9405 - val_loss: 1.6271 - val_accuracy: 0.7060\n",
            "Epoch 126/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1447 - accuracy: 0.9454 - val_loss: 1.5989 - val_accuracy: 0.7082\n",
            "Epoch 127/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1310 - accuracy: 0.9512 - val_loss: 1.5730 - val_accuracy: 0.7071\n",
            "Epoch 128/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1332 - accuracy: 0.9493 - val_loss: 1.6102 - val_accuracy: 0.6982\n",
            "Epoch 129/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1393 - accuracy: 0.9502 - val_loss: 1.6317 - val_accuracy: 0.7024\n",
            "Epoch 130/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1394 - accuracy: 0.9504 - val_loss: 1.5918 - val_accuracy: 0.7071\n",
            "Epoch 131/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1331 - accuracy: 0.9524 - val_loss: 1.5962 - val_accuracy: 0.6979\n",
            "Epoch 132/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1409 - accuracy: 0.9477 - val_loss: 1.5273 - val_accuracy: 0.7058\n",
            "Epoch 133/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1542 - accuracy: 0.9424 - val_loss: 1.5718 - val_accuracy: 0.7058\n",
            "Epoch 134/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1192 - accuracy: 0.9531 - val_loss: 1.6286 - val_accuracy: 0.7035\n",
            "Epoch 135/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1378 - accuracy: 0.9485 - val_loss: 1.5912 - val_accuracy: 0.7003\n",
            "Epoch 136/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1468 - accuracy: 0.9471 - val_loss: 1.5830 - val_accuracy: 0.7034\n",
            "Epoch 137/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1371 - accuracy: 0.9463 - val_loss: 1.5862 - val_accuracy: 0.7019\n",
            "Epoch 138/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1297 - accuracy: 0.9509 - val_loss: 1.5420 - val_accuracy: 0.7097\n",
            "Epoch 139/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1443 - accuracy: 0.9450 - val_loss: 1.5208 - val_accuracy: 0.7032\n",
            "Epoch 140/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1479 - accuracy: 0.9462 - val_loss: 1.5513 - val_accuracy: 0.6961\n",
            "Epoch 141/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1217 - accuracy: 0.9558 - val_loss: 1.5815 - val_accuracy: 0.7082\n",
            "Epoch 142/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1336 - accuracy: 0.9503 - val_loss: 1.5787 - val_accuracy: 0.7013\n",
            "Epoch 143/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1387 - accuracy: 0.9510 - val_loss: 1.5847 - val_accuracy: 0.7045\n",
            "Epoch 144/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1086 - accuracy: 0.9587 - val_loss: 1.5745 - val_accuracy: 0.7124\n",
            "Epoch 145/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1311 - accuracy: 0.9530 - val_loss: 1.5601 - val_accuracy: 0.7111\n",
            "Epoch 146/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1359 - accuracy: 0.9496 - val_loss: 1.5658 - val_accuracy: 0.6916\n",
            "Epoch 147/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1412 - accuracy: 0.9463 - val_loss: 1.5867 - val_accuracy: 0.7119\n",
            "Epoch 148/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1177 - accuracy: 0.9571 - val_loss: 1.6470 - val_accuracy: 0.7032\n",
            "Epoch 149/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1147 - accuracy: 0.9576 - val_loss: 1.6385 - val_accuracy: 0.7093\n",
            "Epoch 150/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1208 - accuracy: 0.9574 - val_loss: 1.6755 - val_accuracy: 0.7026\n",
            "Epoch 151/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1416 - accuracy: 0.9476 - val_loss: 1.5855 - val_accuracy: 0.7071\n",
            "Epoch 152/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1305 - accuracy: 0.9522 - val_loss: 1.6018 - val_accuracy: 0.7145\n",
            "Epoch 153/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.0960 - accuracy: 0.9631 - val_loss: 1.7089 - val_accuracy: 0.6998\n",
            "Epoch 154/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1232 - accuracy: 0.9549 - val_loss: 1.6019 - val_accuracy: 0.7014\n",
            "Epoch 155/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1190 - accuracy: 0.9552 - val_loss: 1.6056 - val_accuracy: 0.7061\n",
            "Epoch 156/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1308 - accuracy: 0.9524 - val_loss: 1.6024 - val_accuracy: 0.7108\n",
            "Epoch 157/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1304 - accuracy: 0.9502 - val_loss: 1.6161 - val_accuracy: 0.7111\n",
            "Epoch 158/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1327 - accuracy: 0.9528 - val_loss: 1.5702 - val_accuracy: 0.6969\n",
            "Epoch 159/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1247 - accuracy: 0.9532 - val_loss: 1.7468 - val_accuracy: 0.7023\n",
            "Epoch 160/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1154 - accuracy: 0.9588 - val_loss: 1.6462 - val_accuracy: 0.7071\n",
            "Epoch 161/200\n",
            "292/292 [==============================] - 1s 5ms/step - loss: 0.1269 - accuracy: 0.9534 - val_loss: 1.6266 - val_accuracy: 0.6968\n",
            "Epoch 162/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1237 - accuracy: 0.9542 - val_loss: 1.6216 - val_accuracy: 0.6997\n",
            "Epoch 163/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1099 - accuracy: 0.9597 - val_loss: 1.6741 - val_accuracy: 0.7106\n",
            "Epoch 164/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1356 - accuracy: 0.9507 - val_loss: 1.6385 - val_accuracy: 0.7048\n",
            "Epoch 165/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1152 - accuracy: 0.9593 - val_loss: 1.6866 - val_accuracy: 0.7103\n",
            "Epoch 166/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1049 - accuracy: 0.9584 - val_loss: 1.7360 - val_accuracy: 0.7005\n",
            "Epoch 167/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1225 - accuracy: 0.9568 - val_loss: 1.6410 - val_accuracy: 0.7045\n",
            "Epoch 168/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1313 - accuracy: 0.9531 - val_loss: 1.6696 - val_accuracy: 0.7042\n",
            "Epoch 169/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1144 - accuracy: 0.9590 - val_loss: 1.6938 - val_accuracy: 0.7082\n",
            "Epoch 170/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1064 - accuracy: 0.9617 - val_loss: 1.6394 - val_accuracy: 0.7064\n",
            "Epoch 171/200\n",
            "292/292 [==============================] - 1s 3ms/step - loss: 0.1046 - accuracy: 0.9607 - val_loss: 1.6976 - val_accuracy: 0.7056\n",
            "Epoch 172/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1343 - accuracy: 0.9509 - val_loss: 1.6393 - val_accuracy: 0.7130\n",
            "Epoch 173/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1211 - accuracy: 0.9570 - val_loss: 1.6874 - val_accuracy: 0.7076\n",
            "Epoch 174/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1278 - accuracy: 0.9534 - val_loss: 1.6273 - val_accuracy: 0.7048\n",
            "Epoch 175/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1049 - accuracy: 0.9622 - val_loss: 1.6883 - val_accuracy: 0.7069\n",
            "Epoch 176/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1121 - accuracy: 0.9616 - val_loss: 1.6952 - val_accuracy: 0.7016\n",
            "Epoch 177/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1159 - accuracy: 0.9575 - val_loss: 1.7598 - val_accuracy: 0.7069\n",
            "Epoch 178/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1054 - accuracy: 0.9611 - val_loss: 1.7025 - val_accuracy: 0.7006\n",
            "Epoch 179/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1172 - accuracy: 0.9571 - val_loss: 1.7343 - val_accuracy: 0.7047\n",
            "Epoch 180/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1418 - accuracy: 0.9496 - val_loss: 1.6013 - val_accuracy: 0.7014\n",
            "Epoch 181/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1021 - accuracy: 0.9633 - val_loss: 1.7501 - val_accuracy: 0.6979\n",
            "Epoch 182/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1010 - accuracy: 0.9630 - val_loss: 1.7720 - val_accuracy: 0.7032\n",
            "Epoch 183/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1278 - accuracy: 0.9539 - val_loss: 1.7334 - val_accuracy: 0.7081\n",
            "Epoch 184/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1032 - accuracy: 0.9631 - val_loss: 1.7189 - val_accuracy: 0.7084\n",
            "Epoch 185/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1168 - accuracy: 0.9569 - val_loss: 1.7529 - val_accuracy: 0.7119\n",
            "Epoch 186/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1141 - accuracy: 0.9594 - val_loss: 1.7853 - val_accuracy: 0.7097\n",
            "Epoch 187/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.0916 - accuracy: 0.9659 - val_loss: 1.8494 - val_accuracy: 0.7039\n",
            "Epoch 188/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1251 - accuracy: 0.9541 - val_loss: 1.7735 - val_accuracy: 0.7002\n",
            "Epoch 189/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1382 - accuracy: 0.9509 - val_loss: 1.6654 - val_accuracy: 0.7076\n",
            "Epoch 190/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1073 - accuracy: 0.9630 - val_loss: 1.6859 - val_accuracy: 0.7081\n",
            "Epoch 191/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.0997 - accuracy: 0.9664 - val_loss: 1.7461 - val_accuracy: 0.7089\n",
            "Epoch 192/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1137 - accuracy: 0.9605 - val_loss: 1.7270 - val_accuracy: 0.7108\n",
            "Epoch 193/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1053 - accuracy: 0.9601 - val_loss: 1.7081 - val_accuracy: 0.7039\n",
            "Epoch 194/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1242 - accuracy: 0.9535 - val_loss: 1.7521 - val_accuracy: 0.7111\n",
            "Epoch 195/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1130 - accuracy: 0.9594 - val_loss: 1.7564 - val_accuracy: 0.7108\n",
            "Epoch 196/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1052 - accuracy: 0.9617 - val_loss: 1.7279 - val_accuracy: 0.7047\n",
            "Epoch 197/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1008 - accuracy: 0.9650 - val_loss: 1.7657 - val_accuracy: 0.7145\n",
            "Epoch 198/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.1158 - accuracy: 0.9588 - val_loss: 1.6870 - val_accuracy: 0.7048\n",
            "Epoch 199/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.0979 - accuracy: 0.9635 - val_loss: 1.7180 - val_accuracy: 0.7132\n",
            "Epoch 200/200\n",
            "292/292 [==============================] - 1s 4ms/step - loss: 0.0901 - accuracy: 0.9689 - val_loss: 1.8093 - val_accuracy: 0.7005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0kNAHQ9nsUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6040312-3e8b-4711-8ac5-26d17b67e714"
      },
      "source": [
        "# Evaluate model training and testing accuracy\n",
        "eval_batch_size = 32\n",
        "train_score, train_accuracy = samuel_900.evaluate(train_data, train_labels, batch_size=eval_batch_size, verbose=0)\n",
        "print('Training loss:', train_score)\n",
        "print(f'\\tTraining accuracy: {train_accuracy}\\n')\n",
        "eval_score, eval_accuracy = samuel_900.evaluate(test_data, test_labels, batch_size=eval_batch_size, verbose=0)\n",
        "print('Testing loss:', eval_score)\n",
        "print(f'\\tTesting accuracy: {eval_accuracy}\\n')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.19711653888225555\n",
            "\tTraining accuracy: 0.9341278672218323\n",
            "\n",
            "Testing loss: 1.8092765808105469\n",
            "\tTesting accuracy: 0.7004830837249756\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5Nc7Y0VpHTK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "3ed026ef-db9d-4273-f9cc-0f96b85ee087"
      },
      "source": [
        "# Plot cost and accuracy, copied from Titantic_2\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcDqPa45jGgp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231ad711-c89c-43e1-c779-190acefc972b"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "samuel_900.save('saved_model/samuel_900')\n",
        "!zip -r /content/saved_model.zip /content/saved_model"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/samuel_900/assets\n",
            "updating: content/saved_model/ (stored 0%)\n",
            "updating: content/saved_model/samuel_900/ (stored 0%)\n",
            "updating: content/saved_model/samuel_900/assets/ (stored 0%)\n",
            "updating: content/saved_model/samuel_900/variables/ (stored 0%)\n",
            "updating: content/saved_model/samuel_900/variables/variables.data-00000-of-00001 (deflated 9%)\n",
            "updating: content/saved_model/samuel_900/variables/variables.index (deflated 68%)\n",
            "updating: content/saved_model/samuel_900/saved_model.pb (deflated 90%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bkaWsKpbf2l"
      },
      "source": [
        "# User-Interface Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8WlweyFjkoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74749a0-f731-46f1-8fbc-b97ffea13679"
      },
      "source": [
        "# Model initialization from notebook virtual machine save\n",
        "samuel_900 = tf.keras.models.load_model('saved_model/samuel_900')\n",
        "samuel_900.summary()     # check its architecture\n",
        "eval_score, eval_accuracy = samuel_900.evaluate(test_data, test_labels, verbose=2)\n",
        "print(f'\\n\\nRestored model score: {eval_score}')\n",
        "print(f'Restored model accuracy: {eval_accuracy}')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"SAMUEL_900\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer_1 (Dense)       (None, 116)               13572     \n",
            "_________________________________________________________________\n",
            "hidden_layer_2 (Dense)       (None, 100)               11700     \n",
            "_________________________________________________________________\n",
            "hidden_layer_3 (Dense)       (None, 80)                8080      \n",
            "_________________________________________________________________\n",
            "hidden_layer_4 (Dense)       (None, 80)                6480      \n",
            "_________________________________________________________________\n",
            "hidden_layer_5 (Dense)       (None, 90)                7290      \n",
            "_________________________________________________________________\n",
            "hidden_layer_6 (Dense)       (None, 50)                4550      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 7)                 357       \n",
            "=================================================================\n",
            "Total params: 52,029\n",
            "Trainable params: 52,029\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "195/195 - 0s - loss: 1.8093 - accuracy: 0.7005\n",
            "\n",
            "\n",
            "Restored model score: 1.8092765808105469\n",
            "Restored model accuracy: 0.7004830837249756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU60yL7hdVv8"
      },
      "source": [
        "def percent_change(old: float, new: float):\n",
        "    if old == 0.0:\n",
        "        old = 0.1\n",
        "    return (new - old) / old\n",
        "\n",
        "\n",
        "def unix_after_bdays(date: datetime, bdays: int) -> int:\n",
        "    date = datetime.today() - BDay(bdays)\n",
        "    return (date - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
        "\n",
        "\n",
        "def get_sample_input(ticker_symbol: str, days_since_last: int=pred_delta_t):\n",
        "    # set days_since_last to 0 for future predictions, pred_delta_t for historical validation\n",
        "    raw_sample_input = []\n",
        "    start_date = unix_after_bdays(datetime.today(), days_since_last+365)\n",
        "    since_last = unix_after_bdays(datetime.today(), days_since_last)\n",
        "    for ti in timeperiod_tis:\n",
        "        for sml_period in sml_period_lengths:\n",
        "            ti_data = fc.technical_indicator(ticker_symbol, 'D', start_date, since_last, indicator=ti, indicator_fields={'timeperiod': sml_period})\n",
        "            if ti == 'aroon':\n",
        "                ti_data = ti_data['aroonup'][-1] - ti_data['aroondown'][-1]\n",
        "            else:\n",
        "                ti_data = ti_data[ti][-1]\n",
        "            raw_sample_input.append(ti_data)\n",
        "            sleep(api_call_time_buffer) # api call buffer\n",
        "    \n",
        "    for ti in no_timeperiod_ti_columns:\n",
        "        ti_data = fc.technical_indicator(ticker_symbol, 'D', start_date, since_last, indicator=ti)\n",
        "        if ti == 'macd':\n",
        "            ti_data = ti_data['macdHist'][-1]\n",
        "        else:\n",
        "            ti_data = ti_data[ti][-1]\n",
        "        raw_sample_input.append(ti_data)\n",
        "        sleep(api_call_time_buffer) # api call buffer\n",
        "\n",
        "    volume = fc.stock_candles(ticker_symbol, 'D', start_date, since_last)['v'][-1]\n",
        "    raw_sample_input.append(volume)\n",
        "\n",
        "    # NOTE: Temporary band-aid for until this can be automated with a stock-index API\n",
        "    # Be careful, there's no user-input safeguards here for simplicity and laziness lol\n",
        "    # for market_name in market_indices:\n",
        "    #     current_price = float(input(f'\\t{market_name}\\'s current price: ').replace(',', ''))\n",
        "    #     for period in sml_period_lengths:\n",
        "    #         period_price = float(input(f'\\t\\t{market_name}\\'s price {period} periods ago: ').replace(',', ''))\n",
        "    #         raw_sample_input.append(percent_change(period_price, current_price))\n",
        "    \n",
        "    # This is the market index data readout for 3/25/2021\n",
        "    # raw_sample_input += [12961.89, 6184470000, 13398.67, 13597.97, 13201.98,  # Nasdaq Composite\n",
        "    #                      2134.27, 4766990000, 2338.54, 2284.38, 2202.98, # Russell 2000\n",
        "    #                      3889.14, 3939.34, 3925.43, 3824.68, # S&P 500 (does NOT have the volume input feature)\n",
        "    #                      32420.06, 3993900, 32485.59, 31961.86, 31097.97] # Dow Jones\n",
        "\n",
        "    # This is the market index data readout for \n",
        "    raw_sample_input += [13533.05, 6435100000, 13777.74, 13543.06, 12377.18,\n",
        "                         2251.07, 5870190000, 2202.42, 2141.42, 1848.70,\n",
        "                         3876.50, 3871.74, 3853.07, 3666.72,\n",
        "                         31521.69, 3872100, 31055.86, 31176.01, 29969.52]\n",
        "\n",
        "    return np.array(raw_sample_input)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy7GBcYTTC49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e7c7488-cb7f-4588-b981-fad29599ce06"
      },
      "source": [
        "loop_title = 'SAMUEL_900 Purchase-Action UI Loop'\n",
        "equal_sign_separator = '=' * len(loop_title)\n",
        "print(f'{loop_title}\\n{equal_sign_separator}')\n",
        "print('Hi! I\\'m Samuel! I give you my predicted likelihoods for whether you should buy, sell, or hold stock for tickers you request!')\n",
        "print('I specialize in tickers in the information technology sector. I was trained using these symbols:')\n",
        "for ticker_symbol in training_symbols:\n",
        "    print(f'\\t{ticker_symbol}')\n",
        "print()\n",
        "\n",
        "print('Here are the latest confidence-magnitudes for buy and sell actions:')\n",
        "for thresh, label in thresh_vec.items():\n",
        "    print(f'{label:<15}{int(thresh)}%')     # assumes that the confidence magnitudes don't have a thousandth place\n",
        "print()\n",
        "print(f'My latest validation accuracy was {round(eval_accuracy*100.0, 2)}%. Random chance is {round(100.0/len(output_columns), 2)}%\\n')\n",
        "exit_code = 'exit0'\n",
        "print('Enter your ticker symbols (not case-sensitive), separated by spaces. More tickers takes more time due to my API-call limits.\\n')\n",
        "final_msg = f'When you\\'re finished, just enter \\'{exit_code}\\' to exit and put me to sleep.'\n",
        "equal_sign_separator = '=' * len(final_msg)\n",
        "print(f'{final_msg}\\n{equal_sign_separator}\\n\\n')\n",
        "\n",
        "output_col_index = -1\n",
        "def next_output_col():\n",
        "    global output_col_index\n",
        "    if output_col_index < len(output_columns)-1:\n",
        "        output_col_index += 1\n",
        "    else:\n",
        "        output_col_index = 0\n",
        "    return output_columns[output_col_index]\n",
        "\n",
        "while True:\n",
        "    symbols = input('What ticker symbols would you like me to analyze?\\n').split()\n",
        "    symbols = list(dict.fromkeys(symbols))      # remove any duplicates\n",
        "    print()\n",
        "    if len(symbols) == 1 and symbols[0] == exit_code:\n",
        "        break\n",
        "    for symbol in symbols:\n",
        "        symbol = symbol.upper()\n",
        "        # try:\n",
        "        #     raw_input_data = get_sample_input(symbol)\n",
        "        # except:\n",
        "        #     print('Something went wrong. You probably entered an unknown ticker symbol...\\n')\n",
        "        #     continue\n",
        "        raw_input_data = get_sample_input(symbol)\n",
        "        print(f'{symbol} profitable-action likelihoods:')\n",
        "        symbol_input_data = scaler.transform(raw_input_data.reshape(1, -1))\n",
        "        prediction = samuel_900.predict(symbol_input_data)[0] * 100.0\n",
        "        for prob in prediction:\n",
        "            prob = str(round(prob, 2)) + '%'\n",
        "            print(f'{prob:<15}{next_output_col()}')\n",
        "        print()\n",
        "print('Goodbye.')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SAMUEL_900 Purchase-Action UI Loop\n",
            "==================================\n",
            "Hi! I'm Samuel! I give you my predicted likelihoods for whether you should buy, sell, or hold stock for tickers you request!\n",
            "I specialize in tickers in the information technology sector. I was trained using these symbols:\n",
            "\tLSCC\n",
            "\tA\n",
            "\tAMZN\n",
            "\tFB\n",
            "\tCSCO\n",
            "\tAAPL\n",
            "\tMSFT\n",
            "\tGOOGL\n",
            "\tORCL\n",
            "\tSAP\n",
            "\tIBM\n",
            "\n",
            "Here are the latest confidence-magnitudes for buy and sell actions:\n",
            "Fair           0%\n",
            "Moderate       0%\n",
            "Strong         0%\n",
            "\n",
            "My latest validation accuracy was 70.05%. Random change is 14.29\n",
            "\n",
            "Enter your ticker symbols (not case-sensitive), separated by spaces. More tickers takes more time due to my API-call limits.\n",
            "\n",
            "When you're finished, just enter 'exit0' to exit and put me to sleep.\n",
            "=====================================================================\n",
            "\n",
            "\n",
            "What ticker symbols would you like me to analyze?\n",
            "aapl\n",
            "\n",
            "AAPL profitable-action likelihoods:\n",
            "0.0%           Strong Sell\n",
            "0.0%           Moderate Sell\n",
            "0.0%           Fair Sell\n",
            "100.0%         Hold\n",
            "0.0%           Fair Buy\n",
            "0.0%           Moderate Buy\n",
            "0.0%           Strong Buy\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-d579f699a441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'What ticker symbols would you like me to analyze?\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0msymbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# remove any duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}